package storage

import (
	"context"
	"fmt"
	"testing"
	"time"

	"deep-coding-agent/internal/context/algorithms"
)

// TestSQLiteIntegration é›†æˆæµ‹è¯•ï¼šSQLite + Context Engine
func TestSQLiteIntegration(t *testing.T) {
	// åˆ›å»ºSQLiteå­˜å‚¨
	config := StorageConfig{
		Type: "sqlite",
		Path: ":memory:",
	}

	storage, err := NewSQLiteStorage(config)
	if err != nil {
		t.Fatalf("Failed to create SQLite storage: %v", err)
	}
	defer storage.Close()

	ctx := context.Background()

	// æµ‹è¯•æ•°æ®ï¼šæŠ€æœ¯æ–‡æ¡£
	testDocs := []Document{
		{
			ID:      "go-concurrency",
			Title:   "Goè¯­è¨€å¹¶å‘ç¼–ç¨‹æŒ‡å—",
			Content: "Goè¯­è¨€é€šè¿‡goroutineå’Œchannelæä¾›äº†å¼ºå¤§çš„å¹¶å‘ç¼–ç¨‹èƒ½åŠ›ã€‚goroutineæ˜¯è½»é‡çº§çš„çº¿ç¨‹ï¼Œchannelç”¨äºgoroutineä¹‹é—´çš„é€šä¿¡ã€‚",
			Metadata: map[string]string{
				"language":  "go",
				"category":  "concurrency",
				"level":     "intermediate",
				"keywords":  "goroutine,channel,concurrency",
			},
			Created: time.Now(),
		},
		{
			ID:      "python-async",
			Title:   "Pythonå¼‚æ­¥ç¼–ç¨‹æœ€ä½³å®è·µ",
			Content: "Pythonçš„asyncioåº“æä¾›äº†å¼‚æ­¥ç¼–ç¨‹æ”¯æŒã€‚ä½¿ç”¨async/awaitè¯­æ³•å¯ä»¥ç¼–å†™é«˜æ•ˆçš„å¼‚æ­¥ä»£ç ï¼Œé¿å…é˜»å¡æ“ä½œã€‚",
			Metadata: map[string]string{
				"language":  "python",
				"category":  "async",
				"level":     "advanced",
				"keywords":  "asyncio,async,await,coroutine",
			},
			Created: time.Now(),
		},
		{
			ID:      "rust-ownership",
			Title:   "Rustæ‰€æœ‰æƒç³»ç»Ÿæ·±åº¦è§£æ",
			Content: "Rustçš„æ‰€æœ‰æƒç³»ç»Ÿæ˜¯å…¶å†…å­˜å®‰å…¨çš„æ ¸å¿ƒã€‚é€šè¿‡æ‰€æœ‰æƒã€å€Ÿç”¨å’Œç”Ÿå‘½å‘¨æœŸï¼ŒRuståœ¨ç¼–è¯‘æ—¶ä¿è¯å†…å­˜å®‰å…¨ã€‚",
			Metadata: map[string]string{
				"language":  "rust",
				"category":  "memory-safety",
				"level":     "expert",
				"keywords":  "ownership,borrowing,lifetime,memory",
			},
			Created: time.Now(),
		},
		{
			ID:      "js-promises",
			Title:   "JavaScript Promiseå’Œå¼‚æ­¥å¤„ç†",
			Content: "JavaScript Promiseæä¾›äº†å¤„ç†å¼‚æ­¥æ“ä½œçš„ä¼˜é›…æ–¹å¼ã€‚ç»“åˆasync/awaitè¯­æ³•ï¼Œå¯ä»¥å†™å‡ºæ›´æ¸…æ™°çš„å¼‚æ­¥ä»£ç ã€‚",
			Metadata: map[string]string{
				"language":  "javascript",
				"category":  "async",
				"level":     "beginner",
				"keywords":  "promise,async,await,callback",
			},
			Created: time.Now(),
		},
		{
			ID:      "db-optimization",
			Title:   "æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–æŠ€å·§",
			Content: "æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–åŒ…æ‹¬ç´¢å¼•è®¾è®¡ã€æŸ¥è¯¢é‡å†™ã€æ‰§è¡Œè®¡åˆ’åˆ†æç­‰ã€‚åˆç†çš„ç´¢å¼•å¯ä»¥å¤§å¹…æå‡æŸ¥è¯¢æ€§èƒ½ã€‚",
			Metadata: map[string]string{
				"category":  "database",
				"level":     "intermediate",
				"keywords":  "optimization,index,query,performance",
			},
			Created: time.Now(),
		},
	}

	// 1. æ‰¹é‡å­˜å‚¨æ–‡æ¡£
	t.Log("ğŸ“š æ‰¹é‡å­˜å‚¨æŠ€æœ¯æ–‡æ¡£...")
	start := time.Now()
	err = storage.BatchStore(ctx, testDocs)
	if err != nil {
		t.Fatalf("Failed to batch store documents: %v", err)
	}
	storeTime := time.Since(start)
	t.Logf("âœ… å­˜å‚¨ %d ä¸ªæ–‡æ¡£è€—æ—¶: %v", len(testDocs), storeTime)

	// 2. ç”Ÿæˆå¹¶å­˜å‚¨å‘é‡
	t.Log("ğŸ”¢ ç”Ÿæˆæ–‡æ¡£å‘é‡...")
	embeddingConfig := algorithms.DefaultEmbeddingConfig()
	
	vectors := make(map[string][]float64)
	for _, doc := range testDocs {
		text := doc.Title + " " + doc.Content
		vector := algorithms.GenerateEmbedding(text, embeddingConfig)
		vectors[doc.ID] = vector
	}

	start = time.Now()
	err = storage.BatchStoreVectors(ctx, vectors)
	if err != nil {
		t.Fatalf("Failed to batch store vectors: %v", err)
	}
	vectorTime := time.Since(start)
	t.Logf("âœ… å­˜å‚¨ %d ä¸ªå‘é‡è€—æ—¶: %v", len(vectors), vectorTime)

	// 3. æµ‹è¯•æ–‡æ¡£æ£€ç´¢
	t.Log("ğŸ” æµ‹è¯•æ–‡æ¡£æ£€ç´¢...")
	
	// æ ¹æ®IDè·å–æ–‡æ¡£
	doc, err := storage.Get(ctx, "go-concurrency")
	if err != nil {
		t.Fatalf("Failed to get document: %v", err)
	}
	t.Logf("ğŸ“„ è·å–æ–‡æ¡£: %s", doc.Title)

	// åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£
	allDocs, err := storage.List(ctx, 10, 0)
	if err != nil {
		t.Fatalf("Failed to list documents: %v", err)
	}
	t.Logf("ğŸ“‹ åˆ—å‡ºæ–‡æ¡£: %d ä¸ª", len(allDocs))

	// 4. æµ‹è¯•å‘é‡ç›¸ä¼¼æœç´¢
	t.Log("ğŸ¯ æµ‹è¯•å‘é‡ç›¸ä¼¼æœç´¢...")
	
	// æœç´¢ä¸"å¹¶å‘ç¼–ç¨‹"ç›¸å…³çš„æ–‡æ¡£
	queryText := "å¹¶å‘ç¼–ç¨‹å’Œå¼‚æ­¥å¤„ç†"
	queryVector := algorithms.GenerateEmbedding(queryText, embeddingConfig)
	
	start = time.Now()
	results, err := storage.SearchSimilar(ctx, queryVector, 3)
	if err != nil {
		t.Fatalf("Failed to search similar vectors: %v", err)
	}
	searchTime := time.Since(start)
	
	t.Logf("ğŸ” æŸ¥è¯¢: \"%s\"", queryText)
	t.Logf("âš¡ æœç´¢è€—æ—¶: %v", searchTime)
	t.Logf("ğŸ“Š æœç´¢ç»“æœ: %d ä¸ª", len(results))
	
	for i, result := range results {
		t.Logf("  %d. %s (ç›¸ä¼¼åº¦: %.3f)", 
			i+1, result.Document.Title, result.Similarity)
	}

	// éªŒè¯æœç´¢ç»“æœè´¨é‡
	if len(results) == 0 {
		t.Error("âŒ æœç´¢åº”è¯¥è¿”å›ç»“æœ")
	}

	// æ£€æŸ¥ç›¸ä¼¼åº¦åˆ†æ•°åˆç†æ€§
	for _, result := range results {
		if result.Similarity < 0 || result.Similarity > 1 {
			t.Errorf("âŒ ç›¸ä¼¼åº¦åˆ†æ•°åº”è¯¥åœ¨0-1ä¹‹é—´ï¼Œå¾—åˆ°: %.3f", result.Similarity)
		}
	}

	// 5. æµ‹è¯•é˜ˆå€¼æœç´¢
	t.Log("ğŸ“ æµ‹è¯•é˜ˆå€¼æœç´¢...")
	thresholdResults, err := storage.SearchByThreshold(ctx, queryVector, 0.3)
	if err != nil {
		t.Fatalf("Failed to search by threshold: %v", err)
	}
	
	t.Logf("ğŸšï¸ é˜ˆå€¼æœç´¢ (>= 0.3): %d ä¸ªç»“æœ", len(thresholdResults))
	for _, result := range thresholdResults {
		if result.Similarity < 0.3 {
			t.Errorf("âŒ é˜ˆå€¼æœç´¢ç»“æœç›¸ä¼¼åº¦åº”è¯¥ >= 0.3ï¼Œå¾—åˆ°: %.3f", result.Similarity)
		}
	}

	// 6. æ€§èƒ½åŸºå‡†æµ‹è¯•
	t.Log("âš¡ æ€§èƒ½åŸºå‡†æµ‹è¯•...")
	
	// æ‰¹é‡æŸ¥è¯¢æ€§èƒ½
	ids := make([]string, len(testDocs))
	for i, doc := range testDocs {
		ids[i] = doc.ID
	}
	
	start = time.Now()
	batchDocs, err := storage.BatchGet(ctx, ids)
	if err != nil {
		t.Fatalf("Failed to batch get documents: %v", err)
	}
	batchTime := time.Since(start)
	
	t.Logf("ğŸ“¦ æ‰¹é‡æŸ¥è¯¢ %d ä¸ªæ–‡æ¡£è€—æ—¶: %v", len(ids), batchTime)
	
	if len(batchDocs) != len(testDocs) {
		t.Errorf("âŒ æ‰¹é‡æŸ¥è¯¢åº”è¯¥è¿”å› %d ä¸ªæ–‡æ¡£ï¼Œå¾—åˆ° %d ä¸ª", len(testDocs), len(batchDocs))
	}

	// 7. å­˜å‚¨ç»Ÿè®¡å’ŒæŒ‡æ ‡
	t.Log("ğŸ“Š å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯...")
	
	count, err := storage.Count(ctx)
	if err != nil {
		t.Fatalf("Failed to count documents: %v", err)
	}
	
	vectorCount := storage.GetVectorCount()
	dimensions := storage.GetDimensions()
	metrics := storage.GetMetrics()
	
	t.Logf("ğŸ“„ æ–‡æ¡£æ€»æ•°: %d", count)
	t.Logf("ğŸ”¢ å‘é‡æ€»æ•°: %d", vectorCount)
	t.Logf("ğŸ“ å‘é‡ç»´åº¦: %d", dimensions)
	t.Logf("ğŸ“ˆ å­˜å‚¨æŒ‡æ ‡:")
	t.Logf("  - è¯»æ“ä½œ: %d æ¬¡", metrics.ReadOps)
	t.Logf("  - å†™æ“ä½œ: %d æ¬¡", metrics.WriteOps)
	t.Logf("  - è¿è¡Œæ—¶é—´: %v", metrics.Uptime)

	// 8. æ•°æ®æ¸…ç†æµ‹è¯•
	t.Log("ğŸ§¹ æµ‹è¯•æ•°æ®æ¸…ç†...")
	
	// åˆ é™¤éƒ¨åˆ†æ–‡æ¡£
	deleteIDs := []string{"js-promises", "db-optimization"}
	err = storage.BatchDelete(ctx, deleteIDs)
	if err != nil {
		t.Fatalf("Failed to batch delete documents: %v", err)
	}
	
	// éªŒè¯åˆ é™¤ç»“æœ
	newCount, err := storage.Count(ctx)
	if err != nil {
		t.Fatalf("Failed to count documents after deletion: %v", err)
	}
	
	expectedCount := count - uint64(len(deleteIDs))
	if newCount != expectedCount {
		t.Errorf("âŒ åˆ é™¤åæ–‡æ¡£æ•°é‡åº”è¯¥æ˜¯ %dï¼Œå¾—åˆ° %d", expectedCount, newCount)
	}
	
	t.Logf("âœ… æˆåŠŸåˆ é™¤ %d ä¸ªæ–‡æ¡£ï¼Œå‰©ä½™ %d ä¸ª", len(deleteIDs), newCount)

	// 9. å¹¶å‘è®¿é—®æµ‹è¯• (ç®€åŒ–ç‰ˆæœ¬)
	t.Log("ğŸ”„ å¹¶å‘è®¿é—®æµ‹è¯•...")
	
	// ç®€å•çš„å¹¶å‘è¯»å†™æµ‹è¯•
	errChan := make(chan error, 3)
	
	// å¹¶å‘è¯»å–
	go func() {
		_, err := storage.Get(ctx, "go-concurrency")
		errChan <- err
	}()
	
	// å¹¶å‘å­˜å‚¨
	go func() {
		concurrentDoc := Document{
			ID:      "concurrent-test-doc",
			Title:   "å¹¶å‘æµ‹è¯•æ–‡æ¡£",
			Content: "å¹¶å‘æµ‹è¯•å†…å®¹",
			Created: time.Now(),
		}
		errChan <- storage.Store(ctx, concurrentDoc)
	}()
	
	// å¹¶å‘æŸ¥è¯¢
	go func() {
		_, err := storage.List(ctx, 5, 0)
		errChan <- err
	}()
	
	// ç­‰å¾…æ‰€æœ‰æ“ä½œå®Œæˆ
	for i := 0; i < 3; i++ {
		if err := <-errChan; err != nil {
			t.Errorf("å¹¶å‘æ“ä½œå¤±è´¥: %v", err)
		}
	}
	
	t.Log("âœ… å¹¶å‘è®¿é—®æµ‹è¯•é€šè¿‡")

	// 10. æœ€ç»ˆéªŒè¯
	t.Log("âœ… é›†æˆæµ‹è¯•å®Œæˆï¼")
	t.Log("==========================================")
	t.Logf("ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
	t.Logf("  - æ–‡æ¡£å­˜å‚¨è€—æ—¶: %v", storeTime)
	t.Logf("  - å‘é‡å­˜å‚¨è€—æ—¶: %v", vectorTime)
	t.Logf("  - æœç´¢å“åº”æ—¶é—´: %v", searchTime)
	t.Logf("  - æ‰¹é‡æŸ¥è¯¢è€—æ—¶: %v", batchTime)
	
	// æ€§èƒ½è¦æ±‚éªŒè¯
	if storeTime > 1*time.Second {
		t.Errorf("âŒ æ–‡æ¡£å­˜å‚¨è¿‡æ…¢: %v > 1s", storeTime)
	}
	
	if searchTime > 100*time.Millisecond {
		t.Errorf("âŒ æœç´¢å“åº”è¿‡æ…¢: %v > 100ms", searchTime)
	}
	
	t.Log("ğŸ‰ SQLiteæ•°æ®åº“é›†æˆæµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
}

// testConcurrentOperationsWithSameStorage ä½¿ç”¨åŒä¸€ä¸ªå­˜å‚¨å®ä¾‹æµ‹è¯•å¹¶å‘æ“ä½œ
func testConcurrentOperationsWithSameStorage(t *testing.T, storage *SQLiteStorage) {
	ctx := context.Background()
	
	const numGoroutines = 5
	const opsPerGoroutine = 10
	
	errChan := make(chan error, numGoroutines)
	
	// å¯åŠ¨å¤šä¸ªgoroutineè¿›è¡Œå¹¶å‘æ“ä½œ
	for i := 0; i < numGoroutines; i++ {
		go func(id int) {
			for j := 0; j < opsPerGoroutine; j++ {
				// å¹¶å‘å­˜å‚¨æ–‡æ¡£
				doc := Document{
					ID:      fmt.Sprintf("concurrent-%d-%d", id, j),
					Title:   fmt.Sprintf("å¹¶å‘æ–‡æ¡£ %d-%d", id, j),
					Content: fmt.Sprintf("å¹¶å‘æµ‹è¯•å†…å®¹ %d-%d", id, j),
					Metadata: map[string]string{
						"goroutine": fmt.Sprintf("%d", id),
						"operation": fmt.Sprintf("%d", j),
					},
					Created: time.Now(),
				}
				
				if err := storage.Store(ctx, doc); err != nil {
					errChan <- fmt.Errorf("concurrent store failed: %w", err)
					return
				}
				
				// å¹¶å‘è¯»å–æ–‡æ¡£
				_, err := storage.Get(ctx, doc.ID)
				if err != nil {
					errChan <- fmt.Errorf("concurrent get failed: %w", err)
					return
				}
			}
			errChan <- nil
		}(i)
	}
	
	// ç­‰å¾…æ‰€æœ‰æ“ä½œå®Œæˆ
	for i := 0; i < numGoroutines; i++ {
		if err := <-errChan; err != nil {
			t.Fatalf("Concurrent operation failed: %v", err)
		}
	}
	
	t.Logf("âœ… å¹¶å‘æ“ä½œæµ‹è¯•é€šè¿‡ï¼š%d goroutines Ã— %d æ“ä½œ", numGoroutines, opsPerGoroutine)
}

// TestSQLiteStorageWithRealData ä½¿ç”¨çœŸå®æ•°æ®çš„æµ‹è¯•
func TestSQLiteStorageWithRealData(t *testing.T) {
	if testing.Short() {
		t.Skip("è·³è¿‡çœŸå®æ•°æ®æµ‹è¯•ï¼ˆä½¿ç”¨ -short æ ‡å¿—ï¼‰")
	}

	config := StorageConfig{
		Type: "sqlite",
		Path: ":memory:",
	}

	storage, err := NewSQLiteStorage(config)
	if err != nil {
		t.Fatalf("Failed to create SQLite storage: %v", err)
	}
	defer storage.Close()

	ctx := context.Background()

	// æ¨¡æ‹Ÿå¤§é‡çœŸå®æ•°æ®
	t.Log("ğŸ—„ï¸ ç”Ÿæˆå¤§é‡æµ‹è¯•æ•°æ®...")
	
	docs := generateLargeDataset(1000) // 1000ä¸ªæ–‡æ¡£
	
	// æ‰¹é‡å­˜å‚¨
	start := time.Now()
	err = storage.BatchStore(ctx, docs)
	if err != nil {
		t.Fatalf("Failed to store large dataset: %v", err)
	}
	storeTime := time.Since(start)
	
	t.Logf("ğŸ“Š å­˜å‚¨ %d ä¸ªæ–‡æ¡£è€—æ—¶: %v (%.2f docs/sec)", 
		len(docs), storeTime, float64(len(docs))/storeTime.Seconds())

	// ç”Ÿæˆå¹¶å­˜å‚¨å‘é‡
	t.Log("ğŸ”¢ æ‰¹é‡ç”Ÿæˆå‘é‡...")
	embeddingConfig := algorithms.DefaultEmbeddingConfig()
	
	vectors := make(map[string][]float64)
	for _, doc := range docs {
		text := doc.Title + " " + doc.Content
		vector := algorithms.GenerateEmbedding(text, embeddingConfig)
		vectors[doc.ID] = vector
	}
	
	start = time.Now()
	err = storage.BatchStoreVectors(ctx, vectors)
	if err != nil {
		t.Fatalf("Failed to store vectors: %v", err)
	}
	vectorTime := time.Since(start)
	
	t.Logf("ğŸ”¢ å­˜å‚¨ %d ä¸ªå‘é‡è€—æ—¶: %v (%.2f vectors/sec)", 
		len(vectors), vectorTime, float64(len(vectors))/vectorTime.Seconds())

	// æµ‹è¯•æœç´¢æ€§èƒ½
	testQueries := []string{
		"æœºå™¨å­¦ä¹ ç®—æ³•",
		"æ•°æ®åº“ä¼˜åŒ–",
		"ç½‘ç»œç¼–ç¨‹",
		"å‰ç«¯å¼€å‘",
		"ç³»ç»Ÿæ¶æ„",
	}

	t.Log("ğŸ” æµ‹è¯•æœç´¢æ€§èƒ½...")
	for _, query := range testQueries {
		queryVector := algorithms.GenerateEmbedding(query, embeddingConfig)
		
		start = time.Now()
		results, err := storage.SearchSimilar(ctx, queryVector, 10)
		if err != nil {
			t.Fatalf("Failed to search for '%s': %v", query, err)
		}
		searchTime := time.Since(start)
		
		t.Logf("  æŸ¥è¯¢ '%s': %d ç»“æœ, è€—æ—¶ %v", query, len(results), searchTime)
		
		// éªŒè¯æœç´¢æ€§èƒ½
		if searchTime > 200*time.Millisecond {
			t.Errorf("æœç´¢æ€§èƒ½ä¸è¾¾æ ‡: %v > 200ms for query '%s'", searchTime, query)
		}
	}

	// å­˜å‚¨ç»Ÿè®¡
	finalMetrics := storage.GetMetrics()
	t.Logf("ğŸ“ˆ æœ€ç»ˆå­˜å‚¨æŒ‡æ ‡:")
	t.Logf("  - æ–‡æ¡£æ•°é‡: %d", finalMetrics.DocumentCount)
	t.Logf("  - è¯»æ“ä½œæ€»æ•°: %d", finalMetrics.ReadOps)
	t.Logf("  - å†™æ“ä½œæ€»æ•°: %d", finalMetrics.WriteOps)
	t.Logf("  - è¿è¡Œæ—¶é—´: %v", finalMetrics.Uptime)
}

// generateLargeDataset ç”Ÿæˆå¤§é‡æµ‹è¯•æ•°æ®
func generateLargeDataset(count int) []Document {
	categories := []string{
		"æœºå™¨å­¦ä¹ ", "æ•°æ®åº“", "ç½‘ç»œç¼–ç¨‹", "å‰ç«¯å¼€å‘", "åç«¯å¼€å‘",
		"ç³»ç»Ÿæ¶æ„", "äº‘è®¡ç®—", "DevOps", "å®‰å…¨", "ç§»åŠ¨å¼€å‘",
	}
	
	languages := []string{
		"Go", "Python", "JavaScript", "Java", "C++", 
		"Rust", "TypeScript", "PHP", "Ruby", "Swift",
	}
	
	docs := make([]Document, count)
	
	for i := 0; i < count; i++ {
		category := categories[i%len(categories)]
		language := languages[i%len(languages)]
		
		docs[i] = Document{
			ID:      fmt.Sprintf("large-doc-%d", i),
			Title:   fmt.Sprintf("%s %så¼€å‘æŒ‡å— %d", language, category, i),
			Content: fmt.Sprintf("è¿™æ˜¯å…³äº%sè¯­è¨€åœ¨%sé¢†åŸŸçš„è¯¦ç»†æŒ‡å—ç¬¬%dç¯‡ã€‚åŒ…å«äº†æœ€ä½³å®è·µã€æ€§èƒ½ä¼˜åŒ–æŠ€å·§ã€å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆç­‰å†…å®¹ã€‚", language, category, i),
			Metadata: map[string]string{
				"category": category,
				"language": language,
				"index":    fmt.Sprintf("%d", i),
				"level":    []string{"beginner", "intermediate", "advanced"}[i%3],
			},
			Created: time.Now().Add(-time.Duration(i) * time.Hour), // æ¨¡æ‹Ÿä¸åŒçš„åˆ›å»ºæ—¶é—´
		}
	}
	
	return docs
}