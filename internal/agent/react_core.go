package agent

import (
	"context"
	"fmt"
	"log"
	"strings"
	"time"

	"deep-coding-agent/internal/llm"
	"deep-coding-agent/pkg/types"
)

// ReactCore - ReActå¾ªç¯çš„æ ¸å¿ƒé€»è¾‘
type ReactCore struct {
	agent *ReactAgent
}

// NewReactCore - åˆ›å»ºReActæ ¸å¿ƒå®ä¾‹
func NewReactCore(agent *ReactAgent) *ReactCore {
	return &ReactCore{agent: agent}
}

// SolveTask - ç»Ÿä¸€çš„ä»»åŠ¡è§£å†³æ–¹æ³•ï¼Œæ”¯æŒæµå¼å’Œéæµå¼
func (rc *ReactCore) SolveTask(ctx context.Context, task string, streamCallback StreamCallback) (*types.LightTaskResult, error) {
	// ç”Ÿæˆä»»åŠ¡ID
	taskID := generateTaskID()

	// åˆå§‹åŒ–ä»»åŠ¡ä¸Šä¸‹æ–‡
	taskCtx := types.NewLightTaskContext(taskID, task)

	// å†³å®šæ˜¯å¦ä½¿ç”¨æµå¼å¤„ç†
	isStreaming := streamCallback != nil
	if isStreaming {
		streamCallback(StreamChunk{Type: "status", Content: "ğŸ§  Starting analysis...", Metadata: map[string]interface{}{"phase": "initialization"}})
	}

	// æ„å»ºåˆå§‹conversation messages (ç”¨äºéæµå¼) æˆ– prompt (ç”¨äºæµå¼)
	var messages []llm.Message
	var prompt string

	if isStreaming {
		prompt = rc.agent.promptBuilder.BuildTaskPrompt(task, taskCtx)
	} else {
		messages = []llm.Message{
			{Role: "user", Content: task},
		}
	}

	// æ‰§è¡ŒReActå¾ªç¯ - é™åˆ¶25æ¬¡è¿­ä»£
	maxIterations := 25
	if isStreaming {
		maxIterations = rc.agent.config.MaxIterations
	}

	for iteration := 1; iteration <= maxIterations; iteration++ {
		step := types.LightExecutionStep{
			Number:    iteration,
			Timestamp: time.Now(),
		}

		// 1. Think Phase (çº¯æ€è€ƒé˜¶æ®µ - ä¸æ¶‰åŠå·¥å…·è°ƒç”¨)
		if isStreaming {
			streamCallback(StreamChunk{
				Type:     "thinking_start",
				Content:  fmt.Sprintf("ğŸ¤” Step %d: Pure thinking and analysis...", iteration),
				Metadata: map[string]interface{}{"iteration": iteration, "phase": "thinking"}})
		}

		var thought string
		var confidence float64
		var thinkTokens int
		var err error

		// çº¯æ€è€ƒé˜¶æ®µï¼šåªåˆ†æï¼Œä¸è°ƒç”¨å·¥å…·
		if isStreaming {
			thought, confidence, thinkTokens, err = rc.agent.thinkingEngine.pureThink(ctx, prompt, taskCtx)
		} else {
			// å¯¹äºéæµå¼æ¨¡å¼ï¼Œæˆ‘ä»¬ä¹Ÿä½¿ç”¨çº¯æ€è€ƒï¼Œä½†éœ€è¦åŸºäºæ¶ˆæ¯å†å²
			conversationPrompt := rc.buildConversationPrompt(messages, taskCtx)
			thought, confidence, thinkTokens, err = rc.agent.thinkingEngine.pureThink(ctx, conversationPrompt, taskCtx)
		}

		if err != nil {
			log.Printf("[ERROR] ReactCore: Think phase failed at iteration %d: %v", iteration, err)
			if isStreaming {
				streamCallback(StreamChunk{Type: "error", Content: fmt.Sprintf("âŒ Thinking failed: %v", err)})
			}
			return nil, fmt.Errorf("thinking failed at iteration %d: %w", iteration, err)
		}

		// Stream the thinking output
		if isStreaming && len(strings.TrimSpace(thought)) > 0 {
			streamCallback(StreamChunk{
				Type:    "thinking_result",
				Content: thought,
				Metadata: map[string]interface{}{
					"iteration":   iteration,
					"confidence":  confidence,
					"tokens_used": thinkTokens,
					"phase":       "pure_reasoning"}})
		}

		step.Thought = thought
		step.Confidence = confidence
		step.TokensUsed = thinkTokens
		taskCtx.TokensUsed += thinkTokens

		// æ£€æŸ¥æ˜¯å¦å¯ä»¥ç›´æ¥æä¾›ç­”æ¡ˆï¼ˆåœ¨è§„åˆ’ä¹‹å‰ï¼‰
		if rc.agent.canProvideDirectAnswer(thought, confidence) {
			step.Action = "provide_answer"
			step.Observation = thought
			step.Duration = time.Since(step.Timestamp)
			taskCtx.History = append(taskCtx.History, step)

			if isStreaming {
				streamCallback(StreamChunk{
					Type:     "final_answer",
					Content:  thought,
					Metadata: map[string]interface{}{"confidence": confidence, "direct_answer": true}})
				streamCallback(StreamChunk{Type: "complete", Content: "âœ… Task completed"})
			}

			return rc.buildFinalResult(taskCtx, thought, confidence, true), nil
		}

		// 2. Plan Phase (è§„åˆ’é˜¶æ®µ - åŸºäºæ€è€ƒç»“æœå†³å®šè¡ŒåŠ¨)
		if isStreaming {
			streamCallback(StreamChunk{
				Type:     "planning_start",
				Content:  fmt.Sprintf("ğŸ“‹ Step %d: Planning actions...", iteration),
				Metadata: map[string]interface{}{"iteration": iteration, "phase": "planning"}})
		}

		actionPlan, planTokens, err := rc.agent.thinkingEngine.planActions(ctx, thought, taskCtx)
		if err != nil {
			log.Printf("[ERROR] ReactCore: Plan phase failed at iteration %d: %v", iteration, err)
			if isStreaming {
				streamCallback(StreamChunk{Type: "error", Content: fmt.Sprintf("âŒ Planning failed: %v", err)})
			}
			return nil, fmt.Errorf("planning failed at iteration %d: %w", iteration, err)
		}

		step.TokensUsed += planTokens
		taskCtx.TokensUsed += planTokens

		// Stream the planning result
		if isStreaming && len(strings.TrimSpace(actionPlan.Reasoning)) > 0 {
			streamCallback(StreamChunk{
				Type:    "planning_result",
				Content: actionPlan.Reasoning,
				Metadata: map[string]interface{}{
					"iteration":        iteration,
					"has_actions":      actionPlan.HasActions,
					"tool_calls_count": len(actionPlan.ToolCalls),
					"has_code":         actionPlan.CodeBlock != nil,
					"tokens_used":      planTokens,
					"phase":            "action_planning"}})
		}

		// å¦‚æœæ²¡æœ‰è§„åˆ’çš„è¡ŒåŠ¨ï¼Œè¯´æ˜ä»»åŠ¡å¯èƒ½å·²ç»å®Œæˆ
		if !actionPlan.HasActions {
			step.Action = "no_action_needed"
			step.Observation = actionPlan.Reasoning
			step.Duration = time.Since(step.Timestamp)
			taskCtx.History = append(taskCtx.History, step)

			if isStreaming {
				streamCallback(StreamChunk{
					Type:     "final_answer",
					Content:  actionPlan.Reasoning,
					Metadata: map[string]interface{}{"confidence": confidence, "no_actions": true}})
				streamCallback(StreamChunk{Type: "complete", Content: "âœ… Task completed"})
			}

			return rc.buildFinalResult(taskCtx, actionPlan.Reasoning, confidence, true), nil
		}

		// 3. Act Phase (æ‰§è¡Œé˜¶æ®µ) - æ‰§è¡Œè§„åˆ’çš„è¡ŒåŠ¨
		if isStreaming {
			streamCallback(StreamChunk{
				Type:     "action_start",
				Content:  fmt.Sprintf("âš¡ Step %d: Executing planned actions...", iteration),
				Metadata: map[string]interface{}{"iteration": iteration, "phase": "executing"}})
		}
		var actionResult *types.LightToolResult

		// æ‰§è¡Œè§„åˆ’çš„å·¥å…·è°ƒç”¨
		if len(actionPlan.ToolCalls) > 0 {
			step.Action = "tool_calls"
			// ä¿å­˜ç¬¬ä¸€ä¸ªå·¥å…·è°ƒç”¨ä¿¡æ¯ç”¨äºæ˜¾ç¤ºï¼ˆå¦‚æœæœ‰å¤šä¸ªï¼Œåªæ˜¾ç¤ºç¬¬ä¸€ä¸ªï¼‰
			if len(actionPlan.ToolCalls) > 0 {
				step.ToolCall = actionPlan.ToolCalls[0]
			}

			if isStreaming {
				actionResult = rc.agent.executeParallelToolsStream(ctx, actionPlan.ToolCalls, streamCallback)
			} else {
				actionResult = rc.agent.executeParallelTools(ctx, actionPlan.ToolCalls)
			}
		} else if actionPlan.CodeBlock != nil {
			// æ‰§è¡Œè§„åˆ’çš„ä»£ç 
			step.Action = "code_execution"

			if isStreaming {
				streamCallback(StreamChunk{Type: "tool_start", Content: fmt.Sprintf("Executing %s code", actionPlan.CodeBlock.Language)})
			}

			actionResult = rc.executeCodeBlock(ctx, actionPlan.CodeBlock, isStreaming, streamCallback)
		} else {
			step.Action = "reasoning"
			if isStreaming {
				streamCallback(StreamChunk{
					Type:     "reasoning_only",
					Content:  actionPlan.Reasoning,
					Metadata: map[string]interface{}{"iteration": iteration, "plan_only": true}})
			}
		}

		step.Result = actionResult

		// å¦‚æœæ‰§è¡Œäº†å·¥å…·ï¼Œå°†å·¥å…·ç»“æœæ·»åŠ åˆ°conversation (éæµå¼æ¨¡å¼)
		if !isStreaming && actionResult != nil && (actionResult.Success || actionResult.Error != "") {
			toolMessages := rc.buildToolMessages(actionResult)
			messages = append(messages, toolMessages...)
		}

		// 4. Observe Phase (è§‚å¯Ÿé˜¶æ®µ)
		if isStreaming {
			streamCallback(StreamChunk{
				Type:     "observation_start",
				Content:  fmt.Sprintf("ğŸ‘ï¸ Step %d: Observing results...", iteration),
				Metadata: map[string]interface{}{"iteration": iteration, "phase": "observing"}})
		}
		var observation string

		if !isStreaming && actionResult != nil {
			// éæµå¼æ¨¡å¼ï¼šè®©æ¨¡å‹è§‚å¯Ÿå’Œåˆ†æå·¥å…·ç»“æœ
			observeResponse, observeThought, _, observeTokens, err := rc.agent.thinkWithConversation(ctx, messages, taskCtx)
			if err != nil {
				observation = rc.agent.observe(actionResult, confidence) // å›é€€åˆ°ç®€å•è§‚å¯Ÿ
			} else {
				observation = observeThought
				taskCtx.TokensUsed += observeTokens
				// å°†è§‚å¯Ÿç»“æœä¹Ÿæ·»åŠ åˆ°conversation
				if observeResponse != nil && len(observeResponse.Choices) > 0 {
					messages = append(messages, llm.Message{
						Role:    "assistant",
						Content: observeResponse.Choices[0].Message.Content,
					})
				}
			}
		} else {
			observation = rc.agent.observe(actionResult, confidence)
		}

		step.Observation = observation
		step.Duration = time.Since(step.Timestamp)

		// Stream the observation/analysis
		if isStreaming && len(strings.TrimSpace(observation)) > 0 {
			streamCallback(StreamChunk{
				Type:    "observation_result",
				Content: observation,
				Metadata: map[string]interface{}{
					"iteration": iteration,
					"duration":  step.Duration.String(),
					"phase":     "analysis"}})
		}

		// æ·»åŠ åˆ°å†å²è®°å½•
		taskCtx.History = append(taskCtx.History, step)
		taskCtx.LastUpdate = time.Now()

		// æ£€æŸ¥å®Œæˆæ¡ä»¶ - åŸºäºæœ€æ–°çš„è§‚å¯Ÿå’Œåˆ†æ

		if rc.agent.isTaskComplete(observation, confidence) {
			if isStreaming {
				streamCallback(StreamChunk{
					Type:    "task_complete",
					Content: observation,
					Metadata: map[string]interface{}{
						"iterations": iteration,
						"confidence": confidence,
						"success":    true}})
				streamCallback(StreamChunk{Type: "complete", Content: "âœ… Task completed successfully"})
			}
			return rc.buildFinalResult(taskCtx, observation, confidence, true), nil
		}

		// å‡†å¤‡ä¸‹ä¸€è½®è¿­ä»£ (æµå¼æ¨¡å¼éœ€è¦é‡æ–°æ„å»ºprompt)
		if isStreaming {
			if rc.agent.config.ContextCompression {
				prompt = rc.agent.contextMgr.CompressContext(taskCtx)
			} else {
				prompt = rc.agent.promptBuilder.BuildTaskPrompt(task, taskCtx)
			}
		}

	}

	// è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
	log.Printf("[WARN] ReactCore: Maximum iterations (%d) reached without completion", maxIterations)
	if isStreaming {
		streamCallback(StreamChunk{
			Type:    "max_iterations",
			Content: fmt.Sprintf("âš ï¸ Reached maximum iterations (%d) without full completion", maxIterations),
			Metadata: map[string]interface{}{
				"max_iterations":     maxIterations,
				"partial_completion": true}})
		streamCallback(StreamChunk{Type: "complete", Content: "âš ï¸ Maximum iterations reached"})
	}
	return rc.buildFinalResult(taskCtx, "Maximum iterations reached without completion", 0.5, false), nil
}

// executeCodeBlock - æ‰§è¡Œä»£ç å—ï¼Œæ”¯æŒæµå¼å’Œéæµå¼
func (rc *ReactCore) executeCodeBlock(ctx context.Context, codeBlock *CodeBlock, isStreaming bool, streamCallback StreamCallback) *types.LightToolResult {
	codeResult, err := rc.agent.codeExecutor.ExecuteCode(ctx, codeBlock.Language, codeBlock.Code)

	if err != nil {
		actionResult := &types.LightToolResult{
			Success: false,
			Error:   err.Error(),
		}
		if isStreaming {
			streamCallback(StreamChunk{Type: "tool_error", Content: err.Error()})
		}
		return actionResult
	}

	actionResult := &types.LightToolResult{
		Success: codeResult.Success,
		Content: codeResult.Output,
		Error:   codeResult.Error,
		Metadata: map[string]interface{}{
			"language":       codeResult.Language,
			"execution_time": codeResult.ExecutionTime,
			"exit_code":      codeResult.ExitCode,
		},
	}

	if isStreaming {
		if codeResult.Success {
			streamCallback(StreamChunk{Type: "tool_result", Content: codeResult.Output})
		} else {
			streamCallback(StreamChunk{Type: "tool_error", Content: codeResult.Error})
		}
	}

	return actionResult
}

// buildToolMessages - æ„å»ºå·¥å…·ç»“æœæ¶ˆæ¯
func (rc *ReactCore) buildToolMessages(actionResult *types.LightToolResult) []llm.Message {
	var toolMessages []llm.Message

	if len(actionResult.ToolCalls) > 0 {
		// å¤„ç†å¤šä¸ªå·¥å…·è°ƒç”¨çš„ç»“æœ
		for _, toolCall := range actionResult.ToolCalls {
			toolMessage := llm.Message{
				Role:       "tool",
				Content:    actionResult.Content,
				ToolCallId: toolCall.CallID,
			}
			if !actionResult.Success {
				toolMessage.Content = actionResult.Error
			}
			toolMessages = append(toolMessages, toolMessage)
		}
	} else {
		// å¤„ç†å•ä¸ªå·¥å…·æˆ–ä»£ç æ‰§è¡Œç»“æœ
		content := actionResult.Content
		if !actionResult.Success {
			content = actionResult.Error
		}
		toolMessages = append(toolMessages, llm.Message{
			Role:    "tool",
			Content: content,
		})
	}

	return toolMessages
}

// buildConversationPrompt - åŸºäºæ¶ˆæ¯å†å²æ„å»ºå¯¹è¯prompt
func (rc *ReactCore) buildConversationPrompt(messages []llm.Message, taskCtx *types.LightTaskContext) string {
	var parts []string

	// æ·»åŠ ä»»åŠ¡ç›®æ ‡
	parts = append(parts, fmt.Sprintf("Task Goal: %s", taskCtx.Goal))

	// æ·»åŠ æ¶ˆæ¯å†å²çš„æ‘˜è¦
	for _, msg := range messages {
		if msg.Role == "user" {
			parts = append(parts, fmt.Sprintf("User: %s", msg.Content))
		} else if msg.Role == "assistant" {
			parts = append(parts, fmt.Sprintf("Assistant: %s", msg.Content))
		}
	}

	// æ·»åŠ æ‰§è¡Œå†å²æ‘˜è¦
	if len(taskCtx.History) > 0 {
		parts = append(parts, "Previous steps:")
		for i, step := range taskCtx.History {
			parts = append(parts, fmt.Sprintf("Step %d: %s", i+1, step.Thought))
		}
	}

	return strings.Join(parts, "\n")
}

// buildFinalResult - æ„å»ºæœ€ç»ˆç»“æœ
func (rc *ReactCore) buildFinalResult(taskCtx *types.LightTaskContext, answer string, confidence float64, success bool) *types.LightTaskResult {
	totalDuration := time.Since(taskCtx.StartTime)

	return &types.LightTaskResult{
		Success:    success,
		Answer:     answer,
		Confidence: confidence,
		Steps:      taskCtx.History,
		Duration:   totalDuration,
		TokensUsed: taskCtx.TokensUsed,
	}
}
