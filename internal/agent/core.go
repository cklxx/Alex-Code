package agent

import (
	"context"
	"fmt"
	"log"
	"strings"
	"time"

	contextmgr "alex/internal/context"
	"alex/internal/llm"
	"alex/internal/memory"
	"alex/internal/session"
	"alex/pkg/types"
)

// ReactCore - ä½¿ç”¨å·¥å…·è°ƒç”¨æµç¨‹çš„ReactCoreæ ¸å¿ƒå®ç°
type ReactCore struct {
	agent            *ReactAgent
	streamCallback   StreamCallback
	messageProcessor *MessageProcessor
	llmHandler       *LLMHandler
	toolHandler      *ToolHandler
	promptHandler    *PromptHandler
}

// NewReactCore - åˆ›å»ºReActæ ¸å¿ƒå®ä¾‹
func NewReactCore(agent *ReactAgent) *ReactCore {
	llmClient, err := llm.GetLLMInstance(llm.BasicModel)
	if err != nil {
		log.Printf("[ERROR] NewReactCore: Failed to get LLM instance: %v", err)
		llmClient = nil
	}

	return &ReactCore{
		agent:            agent,
		messageProcessor: NewMessageProcessor(llmClient, agent.sessionManager),
		llmHandler:       NewLLMHandler(nil), // Will be set per request
		toolHandler:      NewToolHandler(agent.tools),
		promptHandler:    NewPromptHandler(agent.promptBuilder),
	}
}

// SolveTask - ä½¿ç”¨å·¥å…·è°ƒç”¨æµç¨‹çš„ç®€åŒ–ä»»åŠ¡è§£å†³æ–¹æ³•
func (rc *ReactCore) SolveTask(ctx context.Context, task string, streamCallback StreamCallback) (*types.ReactTaskResult, error) {
	// è®¾ç½®æµå›è°ƒ
	rc.streamCallback = streamCallback
	rc.llmHandler.streamCallback = streamCallback

	// ç”Ÿæˆä»»åŠ¡ID
	taskID := generateTaskID()

	// åˆå§‹åŒ–ä»»åŠ¡ä¸Šä¸‹æ–‡
	taskCtx := types.NewReactTaskContext(taskID, task)

	// å†³å®šæ˜¯å¦ä½¿ç”¨æµå¼å¤„ç†
	isStreaming := streamCallback != nil
	if isStreaming {
		streamCallback(StreamChunk{Type: "status", Content: GetRandomProcessingMessage(), Metadata: map[string]any{"phase": "initialization"}})
	}

	// æ„å»ºç³»ç»Ÿæç¤ºï¼ˆåªéœ€æ„å»ºä¸€æ¬¡ï¼‰
	systemPrompt := rc.promptHandler.buildToolDrivenTaskPrompt(taskCtx)
	messages := []llm.Message{
		{
			Role:    "system",
			Content: systemPrompt,
		},
	}

	// æ‰§è¡Œå·¥å…·é©±åŠ¨çš„ReActå¾ªç¯
	maxIterations := 25 // å‡å°‘è¿­ä»£æ¬¡æ•°ï¼Œä¾èµ–æ™ºèƒ½å·¥å…·è°ƒç”¨

	for iteration := 1; iteration <= maxIterations; iteration++ {
		step := types.ReactExecutionStep{
			Number:    iteration,
			Timestamp: time.Now(),
		}

		if isStreaming {
			streamCallback(StreamChunk{
				Type:     "iteration",
				Content:  fmt.Sprintf("ğŸ”„ Iteration %d: Processing with tool-driven approach...", iteration),
				Metadata: map[string]any{"iteration": iteration, "phase": "tool_driven_processing"}})
		}

		// ç¬¬ä¸€æ¬¡è¿­ä»£æ›´æ–°æ¶ˆæ¯åˆ—è¡¨ï¼Œæ·»åŠ æœ€æ–°çš„ä¼šè¯å†…å®¹
		if iteration == 1 {
			sess := rc.messageProcessor.GetCurrentSession(ctx, rc.agent)
			// ä½¿ç”¨æ–°çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¼˜åŒ–æ¶ˆæ¯
			sessionMessages := sess.GetMessages()
			optimizedMessages, err := rc.agent.contextManager.OptimizeContext(ctx, sess.ID, sessionMessages)
			if err != nil {
				log.Printf("[WARN] Context optimization failed: %v", err)
				optimizedMessages = sessionMessages
			}
			llmMessages := rc.messageProcessor.ConvertSessionToLLM(optimizedMessages)
			messages = append(messages, llmMessages...)
		} else {
			sessionMessages := rc.messageProcessor.ConvertLLMToSession(messages)
			sessionMessages = rc.messageProcessor.compressMessages(sessionMessages)
			messages = rc.messageProcessor.ConvertSessionToLLM(sessionMessages)
		}
		// æ„å»ºå¯ç”¨å·¥å…·åˆ—è¡¨ - æ¯è½®éƒ½åŒ…å«å·¥å…·å®šä¹‰ä»¥ç¡®ä¿æ¨¡å‹èƒ½è°ƒç”¨å·¥å…·
		tools := rc.toolHandler.buildToolDefinitions()

		request := &llm.ChatRequest{
			Messages:   messages,
			ModelType:  llm.BasicModel,
			Tools:      tools,
			ToolChoice: "auto",
			Config:     rc.agent.llmConfig,
			MaxTokens:  rc.agent.llmConfig.MaxTokens,
		}
		// è·å–LLMå®ä¾‹
		client, err := llm.GetLLMInstance(llm.BasicModel)
		if err != nil {
			log.Printf("[ERROR] ReactCore: Failed to get LLM instance at iteration %d: %v", iteration, err)
			if isStreaming {
				streamCallback(StreamChunk{Type: "error", Content: fmt.Sprintf("âŒ LLM initialization failed: %v", err)})
			}
			return nil, fmt.Errorf("LLM initialization failed at iteration %d: %w", iteration, err)
		}

		// æ·»åŠ è¯·æ±‚å‚æ•°éªŒè¯
		if err := rc.llmHandler.validateLLMRequest(request); err != nil {
			log.Printf("[ERROR] ReactCore: Invalid LLM request at iteration %d: %v", iteration, err)
			if isStreaming {
				streamCallback(StreamChunk{Type: "error", Content: fmt.Sprintf("âŒ Invalid request: %v", err)})
			}
			return nil, fmt.Errorf("invalid LLM request at iteration %d: %w", iteration, err)
		}

		// æ‰§è¡ŒLLMè°ƒç”¨ï¼Œå¸¦é‡è¯•æœºåˆ¶
		response, err := rc.llmHandler.callLLMWithRetry(ctx, client, request, 3)
		if err != nil {
			log.Printf("[ERROR] ReactCore: LLM call failed at iteration %d after retries: %v", iteration, err)
			if isStreaming {
				streamCallback(StreamChunk{Type: "error", Content: fmt.Sprintf("âŒ LLM call failed: %v", err)})
			}
			return nil, fmt.Errorf("LLM call failed at iteration %d: %w", iteration, err)
		}

		// å¢å¼ºçš„å“åº”éªŒè¯
		if response == nil {
			err := fmt.Errorf("received nil response from LLM at iteration %d", iteration)
			log.Printf("[ERROR] ReactCore: %v", err)
			if isStreaming {
				streamCallback(StreamChunk{Type: "error", Content: "âŒ Received empty response from LLM"})
			}
			return nil, err
		}

		if len(response.Choices) == 0 {
			log.Printf("[ERROR] ReactCore: No response choices at iteration %d", iteration)
			if isStreaming {
				streamCallback(StreamChunk{Type: "error", Content: "âŒ No response choices from LLM - API response format issue"})
			}
			return nil, fmt.Errorf("no response choices received at iteration %d - API response format issue", iteration)
		}

		log.Printf("DEBUG: Response: %+v", response)
		choice := response.Choices[0]
		step.Thought = strings.TrimSpace(choice.Message.Content)

		// Extract token usage from response using compatible method
		usage := response.GetUsage()
		tokensUsed := usage.GetTotalTokens()
		promptTokens := usage.GetPromptTokens()
		completionTokens := usage.GetCompletionTokens()

		// Update task context with token usage
		taskCtx.TokensUsed += tokensUsed
		taskCtx.PromptTokens += promptTokens
		taskCtx.CompletionTokens += completionTokens
		step.TokensUsed = tokensUsed

		// Send token usage via stream callback
		if isStreaming && tokensUsed > 0 {
			streamCallback(StreamChunk{
				Type:             "token_usage",
				Content:          fmt.Sprintf("Tokens used: %d (prompt: %d, completion: %d)", tokensUsed, promptTokens, completionTokens),
				TokensUsed:       tokensUsed,
				TotalTokensUsed:  taskCtx.TokensUsed,
				PromptTokens:     promptTokens,
				CompletionTokens: completionTokens,
				Metadata:         map[string]any{"iteration": iteration, "phase": "token_accounting"},
			})
		}

		if len(choice.Message.Content) > 0 && len(choice.Message.ToolCalls) > 0 {
			streamCallback(StreamChunk{
				Type:     "thinking_result",
				Content:  choice.Message.Content,
				Metadata: map[string]any{"iteration": iteration, "phase": "thinking_result"}})
		}
		// æ·»åŠ assistantæ¶ˆæ¯åˆ°å¯¹è¯å†å²å’Œsession
		// é‡è¦ä¿®å¤ï¼šå³ä½¿æ²¡æœ‰contentï¼Œä¹Ÿè¦æ·»åŠ åŒ…å«å·¥å…·è°ƒç”¨çš„assistantæ¶ˆæ¯
		if len(choice.Message.Content) > 0 || len(choice.Message.ToolCalls) > 0 {
			log.Printf("[DEBUG] ReactCore: Adding assistant message - Content length: %d, ToolCalls: %d", len(choice.Message.Content), len(choice.Message.ToolCalls))
			messages = append(messages, choice.Message)
			// åŒæ—¶æ·»åŠ åˆ°sessionä»¥ä¾›memoryç³»ç»Ÿå­¦ä¹ 
			rc.addMessageToSession(ctx, &choice.Message)
		}

		// è§£æå¹¶æ‰§è¡Œå·¥å…·è°ƒç”¨
		toolCalls := rc.agent.parseToolCalls(&choice.Message)
		log.Printf("[DEBUG] ReactCore: Parsed %d tool calls", len(toolCalls))

		// è®°å½•æ‰€æœ‰ä»LLMæ¥æ”¶åˆ°çš„å·¥å…·è°ƒç”¨IDï¼Œç”¨äºéªŒè¯å“åº”å®Œæ•´æ€§
		expectedToolCallIDs := make([]string, 0, len(choice.Message.ToolCalls))
		for _, tc := range choice.Message.ToolCalls {
			expectedToolCallIDs = append(expectedToolCallIDs, tc.ID)
			log.Printf("[DEBUG] ReactCore: Expected tool call ID: %s", tc.ID)
		}

		if len(toolCalls) > 0 {
			step.Action = "tool_execution"
			step.ToolCall = toolCalls // è®°å½•æ‰€æœ‰å·¥å…·è°ƒç”¨

			// æ‰§è¡Œå·¥å…·è°ƒç”¨
			toolResult := rc.agent.executeSerialToolsStream(ctx, toolCalls, streamCallback)
			step.Result = toolResult

			log.Printf("[DEBUG] ReactCore: Tool execution returned %d results", len(toolResult))
			for i, result := range toolResult {
				log.Printf("[DEBUG] ReactCore: Tool result %d - Tool: '%s', CallID: '%s', Success: %v", i, result.ToolName, result.CallID, result.Success)
			}

			// å°†å·¥å…·ç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²å’Œsession
			if toolResult != nil {
				isGemini := strings.Contains(request.Config.BaseURL, "googleapis")
				log.Printf("[DEBUG] ReactCore: Building tool messages, isGemini: %v", isGemini)
				toolMessages := rc.toolHandler.buildToolMessages(toolResult, isGemini)
				log.Printf("[DEBUG] ReactCore: Built %d tool messages", len(toolMessages))

				for i, msg := range toolMessages {
					log.Printf("[DEBUG] ReactCore: Tool message %d - Role: '%s', ToolCallId: '%s'", i, msg.Role, msg.ToolCallId)
				}

				// éªŒè¯å“åº”å®Œæ•´æ€§ï¼šç¡®ä¿æ¯ä¸ªæœŸæœ›çš„å·¥å…·è°ƒç”¨IDéƒ½æœ‰å¯¹åº”çš„å“åº”
				receivedIDs := make(map[string]bool)
				for _, msg := range toolMessages {
					if msg.ToolCallId != "" {
						receivedIDs[msg.ToolCallId] = true
					}
				}

				// æ£€æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±çš„å“åº”
				var missingIDs []string
				for _, expectedID := range expectedToolCallIDs {
					if !receivedIDs[expectedID] {
						missingIDs = append(missingIDs, expectedID)
					}
				}

				// å¦‚æœæœ‰ç¼ºå¤±çš„IDï¼Œç”Ÿæˆfallbackå“åº” - åŠ å¼ºé”™è¯¯å¤„ç†
				if len(missingIDs) > 0 {
					log.Printf("[ERROR] ReactCore: Missing responses for tool call IDs: %v", missingIDs)
					log.Printf("[ERROR] ReactCore: Expected IDs: %v, Received IDs: %v", expectedToolCallIDs, func() []string {
						var received []string
						for id := range receivedIDs {
							received = append(received, id)
						}
						return received
					}())

					for _, missingID := range missingIDs {
						// å°è¯•æ‰¾åˆ°å¯¹åº”çš„å·¥å…·åç§°
						var toolName = "unknown"
						for _, tc := range choice.Message.ToolCalls {
							if tc.ID == missingID {
								toolName = tc.Function.Name
								break
							}
						}

						fallbackMsg := llm.Message{
							Role:       "tool",
							Content:    fmt.Sprintf("Tool execution failed: no response generated for %s", toolName),
							ToolCallId: missingID,
							Name:       toolName,
						}
						toolMessages = append(toolMessages, fallbackMsg)
						log.Printf("[ERROR] ReactCore: Generated fallback response for missing ID: %s (tool: %s)", missingID, toolName)
					}

					// å¦‚æœæœ‰ç¼ºå¤±å“åº”ï¼Œé€šè¿‡æµå›è°ƒé€šçŸ¥ç”¨æˆ·
					if isStreaming {
						streamCallback(StreamChunk{
							Type:     "tool_error",
							Content:  fmt.Sprintf("Warning: %d tool call(s) failed to generate proper responses", len(missingIDs)),
							Metadata: map[string]any{"missing_tool_calls": missingIDs},
						})
					}
				}

				messages = append(messages, toolMessages...)

				// å°†å·¥å…·æ¶ˆæ¯æ·»åŠ åˆ°sessionä¾›memoryç³»ç»Ÿå­¦ä¹ 
				rc.addToolMessagesToSession(ctx, toolMessages, toolResult)

				step.Observation = rc.toolHandler.generateObservation(toolResult)
			}
		} else {
			finalAnswer := choice.Message.Content

			step.Action = "direct_answer"
			step.Observation = finalAnswer
			step.Duration = time.Since(step.Timestamp)
			taskCtx.History = append(taskCtx.History, step)

			result := buildFinalResult(taskCtx, finalAnswer, 0.8, true)
			result.TokensUsed = taskCtx.TokensUsed

			if isStreaming {
				streamCallback(StreamChunk{
					Type:     "final_answer",
					Content:  finalAnswer,
					Metadata: map[string]any{"iteration": iteration, "phase": "final_answer"}})
			}
			return result, nil
		}

		step.Duration = time.Since(step.Timestamp)
		taskCtx.History = append(taskCtx.History, step)
		taskCtx.LastUpdate = time.Now()
	}

	// è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
	log.Printf("[WARN] ReactCore: Maximum iterations (%d) reached", maxIterations)
	if isStreaming {
		streamCallback(StreamChunk{
			Type:     "max_iterations",
			Content:  fmt.Sprintf("âš ï¸ Reached maximum iterations (%d)", maxIterations),
			Metadata: map[string]any{"max_iterations": maxIterations}})
	}

	return buildFinalResult(taskCtx, "Maximum iterations reached without completion", 0.5, false), nil
}

// GetContextStats - è·å–ä¸Šä¸‹æ–‡ç»Ÿè®¡ä¿¡æ¯
func (rc *ReactCore) GetContextStats(sess *session.Session) *contextmgr.ContextStats {
	return rc.messageProcessor.GetContextStats(sess)
}

// RestoreFullContext - æ¢å¤å®Œæ•´ä¸Šä¸‹æ–‡
func (rc *ReactCore) RestoreFullContext(sess *session.Session, backupID string) error {
	return rc.messageProcessor.RestoreFullContext(sess, backupID)
}

// addMessageToSession - å°†LLMæ¶ˆæ¯æ·»åŠ åˆ°sessionä¸­ä¾›memoryç³»ç»Ÿå­¦ä¹ 
func (rc *ReactCore) addMessageToSession(ctx context.Context, llmMsg *llm.Message) {
	// è·å–å½“å‰ä¼šè¯
	sess := rc.messageProcessor.GetCurrentSession(ctx, rc.agent)
	if sess == nil {
		return // æ²¡æœ‰ä¼šè¯åˆ™è·³è¿‡
	}

	// è½¬æ¢LLMæ¶ˆæ¯ä¸ºsessionæ¶ˆæ¯æ ¼å¼
	sessionMsg := &session.Message{
		Role:      llmMsg.Role,
		Content:   llmMsg.Content,
		Timestamp: time.Now(),
		Metadata: map[string]interface{}{
			"source":    "llm_response",
			"timestamp": time.Now().Unix(),
		},
	}

	// è½¬æ¢å·¥å…·è°ƒç”¨ä¿¡æ¯
	if len(llmMsg.ToolCalls) > 0 {
		for _, tc := range llmMsg.ToolCalls {
			// å°†Argumentså­—ç¬¦ä¸²è§£æä¸ºmap[string]interface{}
			var args map[string]interface{}
			if tc.Function.Arguments != "" {
				// ç®€å•å¤„ç†ï¼šå¦‚æœæ˜¯JSONå­—ç¬¦ä¸²å°è¯•è§£æï¼Œå¦åˆ™å­˜ä¸ºå­—ç¬¦ä¸²
				args = map[string]interface{}{"raw": tc.Function.Arguments}
			}

			sessionMsg.ToolCalls = append(sessionMsg.ToolCalls, session.ToolCall{
				ID:   tc.ID,
				Name: tc.Function.Name,
				Args: args,
			})
		}
		sessionMsg.Metadata["has_tool_calls"] = true
		sessionMsg.Metadata["tool_count"] = len(llmMsg.ToolCalls)
	}

	// æ·»åŠ åˆ°session
	sess.AddMessage(sessionMsg)
}

// addToolMessagesToSession - å°†å·¥å…·æ¶ˆæ¯æ·»åŠ åˆ°sessionä¸­ä¾›memoryç³»ç»Ÿå­¦ä¹ 
func (rc *ReactCore) addToolMessagesToSession(ctx context.Context, toolMessages []llm.Message, toolResults []*types.ReactToolResult) {
	// è·å–å½“å‰ä¼šè¯
	sess := rc.messageProcessor.GetCurrentSession(ctx, rc.agent)
	if sess == nil {
		return // æ²¡æœ‰ä¼šè¯åˆ™è·³è¿‡
	}

	// å¤„ç†æ¯ä¸ªå·¥å…·æ¶ˆæ¯
	for _, toolMsg := range toolMessages {
		sessionMsg := &session.Message{
			Role:      toolMsg.Role,
			Content:   toolMsg.Content,
			Timestamp: time.Now(),
			Metadata: map[string]interface{}{
				"source":    "tool_result",
				"timestamp": time.Now().Unix(),
			},
		}

		// ä¿å­˜tool_call_idåˆ°metadataä¸­ - è¿™æ˜¯å…³é”®ä¿®å¤
		if toolMsg.ToolCallId != "" {
			sessionMsg.Metadata["tool_call_id"] = toolMsg.ToolCallId
		}

		// å¦‚æœæ˜¯å·¥å…·ç»“æœæ¶ˆæ¯ï¼Œæ·»åŠ é¢å¤–çš„å…ƒæ•°æ®
		if toolMsg.Role == "tool" && len(toolResults) > 0 {
			// å°è¯•åŒ¹é…å¯¹åº”çš„å·¥å…·ç»“æœ
			for _, result := range toolResults {
				if result != nil && toolMsg.ToolCallId == result.CallID {
					sessionMsg.Metadata["tool_name"] = result.ToolName
					sessionMsg.Metadata["tool_success"] = result.Success
					sessionMsg.Metadata["execution_time"] = result.Duration.Milliseconds()
					if result.Error != "" {
						sessionMsg.Metadata["tool_error"] = result.Error
					}
					break
				}
			}
		}

		// æ·»åŠ åˆ°session
		sess.AddMessage(sessionMsg)
	}

	// å¼‚æ­¥åˆ›å»ºå·¥å…·ä½¿ç”¨ç›¸å…³çš„memory
	if rc.agent.memoryManager != nil && len(toolResults) > 0 {
		go rc.createToolUsageMemory(sess.ID, toolResults)
	}
}

// createToolUsageMemory - åˆ›å»ºå·¥å…·ä½¿ç”¨ç›¸å…³çš„è®°å¿†
func (rc *ReactCore) createToolUsageMemory(sessionID string, toolResults []*types.ReactToolResult) {
	defer func() {
		if r := recover(); r != nil {
			log.Printf("[ERROR] Tool usage memory creation panic: %v", r)
		}
	}()

	if rc.agent.memoryManager == nil {
		return
	}

	// ç»Ÿè®¡å·¥å…·ä½¿ç”¨æƒ…å†µ
	var successfulTools, failedTools []string
	totalTime := time.Duration(0)

	for _, result := range toolResults {
		if result == nil {
			continue
		}

		if result.Success {
			successfulTools = append(successfulTools, result.ToolName)
		} else {
			failedTools = append(failedTools, result.ToolName)
		}
		totalTime += result.Duration
	}

	// åˆ›å»ºå·¥å…·ä½¿ç”¨æ¨¡å¼è®°å¿†
	if len(successfulTools) > 0 {
		memory := &memory.MemoryItem{
			ID:         fmt.Sprintf("tool_usage_%s_%d", sessionID, time.Now().UnixNano()),
			SessionID:  sessionID,
			Category:   memory.TaskHistory,
			Content:    fmt.Sprintf("Successfully used tools: %s (total time: %v)", strings.Join(successfulTools, ", "), totalTime),
			Importance: 0.7,
			Tags:       append([]string{"tool_usage", "success"}, successfulTools...),
			CreatedAt:  time.Now(),
			UpdatedAt:  time.Now(),
			LastAccess: time.Now(),
			Metadata: map[string]interface{}{
				"successful_tools": successfulTools,
				"execution_time":   totalTime.Milliseconds(),
				"tool_count":       len(successfulTools),
			},
		}

		if err := rc.agent.memoryManager.Store(memory); err != nil {
			log.Printf("[WARN] Failed to store tool usage memory: %v", err)
		}
	}

	// åˆ›å»ºå·¥å…·å¤±è´¥è®°å¿†
	if len(failedTools) > 0 {
		memory := &memory.MemoryItem{
			ID:         fmt.Sprintf("tool_failure_%s_%d", sessionID, time.Now().UnixNano()),
			SessionID:  sessionID,
			Category:   memory.ErrorPatterns,
			Content:    fmt.Sprintf("Failed tools: %s", strings.Join(failedTools, ", ")),
			Importance: 0.8, // å¤±è´¥è®°å¿†æ›´é‡è¦ï¼Œç”¨äºé¿å…é‡å¤é”™è¯¯
			Tags:       append([]string{"tool_failure", "error"}, failedTools...),
			CreatedAt:  time.Now(),
			UpdatedAt:  time.Now(),
			LastAccess: time.Now(),
			Metadata: map[string]interface{}{
				"failed_tools":  failedTools,
				"failure_count": len(failedTools),
			},
		}

		if err := rc.agent.memoryManager.Store(memory); err != nil {
			log.Printf("[WARN] Failed to store tool failure memory: %v", err)
		}
	}
}
