package agent

import (
	"context"
	"fmt"
	"log"
	"strings"
	"time"

	"deep-coding-agent/internal/llm"
	"deep-coding-agent/pkg/types"
)

// ReactCore - ä½¿ç”¨å·¥å…·è°ƒç”¨æµç¨‹çš„ReactCoreæ ¸å¿ƒå®ç°
type ReactCore struct {
	agent          *ReactAgent
	streamCallback StreamCallback // å½“å‰æµå›è°ƒ
}

// NewReactCore - åˆ›å»ºReActæ ¸å¿ƒå®ä¾‹
func NewReactCore(agent *ReactAgent) *ReactCore {
	return &ReactCore{agent: agent}
}

// SolveTask - ä½¿ç”¨å·¥å…·è°ƒç”¨æµç¨‹çš„ç®€åŒ–ä»»åŠ¡è§£å†³æ–¹æ³•
func (rc *ReactCore) SolveTask(ctx context.Context, task string, streamCallback StreamCallback) (*types.ReactTaskResult, error) {
	// è®¾ç½®æµå›è°ƒ
	rc.streamCallback = streamCallback

	// ç”Ÿæˆä»»åŠ¡ID
	taskID := generateTaskID()

	// åˆå§‹åŒ–ä»»åŠ¡ä¸Šä¸‹æ–‡
	taskCtx := types.NewReactTaskContext(taskID, task)

	// å†³å®šæ˜¯å¦ä½¿ç”¨æµå¼å¤„ç†
	isStreaming := streamCallback != nil
	if isStreaming {
		streamCallback(StreamChunk{Type: "status", Content: "ğŸ§  Starting tool-driven ReAct process...", Metadata: map[string]interface{}{"phase": "initialization"}})
	}

	// ä½¿ç”¨ç®€åŒ–çš„ç³»ç»Ÿæ¶ˆæ¯ï¼Œé¿å…tokenè¿‡å¤šå¯¼è‡´APIé”™è¯¯
	messages := []llm.Message{
		{Role: "system", Content: rc.buildToolDrivenTaskPrompt(task)},
		{Role: "system", Content: rc.agent.contextMgr.CompressContext(taskCtx)},
		{Role: "user", Content: task},
	}

	// æ‰§è¡Œå·¥å…·é©±åŠ¨çš„ReActå¾ªç¯
	maxIterations := 10 // å‡å°‘è¿­ä»£æ¬¡æ•°ï¼Œä¾èµ–æ™ºèƒ½å·¥å…·è°ƒç”¨

	for iteration := 1; iteration <= maxIterations; iteration++ {
		step := types.ReactExecutionStep{
			Number:    iteration,
			Timestamp: time.Now(),
		}

		if isStreaming {
			streamCallback(StreamChunk{
				Type:     "iteration",
				Content:  fmt.Sprintf("ğŸ”„ Iteration %d: Processing with tool-driven approach...", iteration),
				Metadata: map[string]interface{}{"iteration": iteration, "phase": "tool_driven_processing"}})
		}

		// æ„å»ºå¯ç”¨å·¥å…·åˆ—è¡¨ - ä»…åœ¨ç¬¬ä¸€è½®åŒ…å«æ‰€æœ‰å·¥å…·ï¼Œåç»­è½®æ¬¡ä¸åŒ…å«å·¥å…·å®šä¹‰
		var tools []llm.Tool
		var toolChoice string

		if iteration == 1 {
			tools = rc.buildToolDefinitions()
			toolChoice = "auto"
		} else {
			// åç»­è½®æ¬¡ä¸åŒ…å«å·¥å…·å®šä¹‰ï¼Œé¿å…tokenè¿‡å¤š
			tools = nil
			toolChoice = ""
		}

		request := &llm.ChatRequest{
			Messages:   messages,
			ModelType:  llm.BasicModel,
			Tools:      tools,
			ToolChoice: toolChoice,
			Config:     rc.agent.llmConfig,
		}

		// è·å–LLMå®ä¾‹
		client, err := llm.GetLLMInstance(llm.BasicModel)
		if err != nil {
			log.Printf("[ERROR] ReactCore: Failed to get LLM instance at iteration %d: %v", iteration, err)
			if isStreaming {
				streamCallback(StreamChunk{Type: "error", Content: fmt.Sprintf("âŒ LLM initialization failed: %v", err)})
			}
			return nil, fmt.Errorf("LLM initialization failed at iteration %d: %w", iteration, err)
		}

		// æ·»åŠ è¯·æ±‚å‚æ•°éªŒè¯
		if err := rc.validateLLMRequest(request); err != nil {
			log.Printf("[ERROR] ReactCore: Invalid LLM request at iteration %d: %v", iteration, err)
			if isStreaming {
				streamCallback(StreamChunk{Type: "error", Content: fmt.Sprintf("âŒ Invalid request: %v", err)})
			}
			return nil, fmt.Errorf("invalid LLM request at iteration %d: %w", iteration, err)
		}

		// æ‰§è¡ŒLLMè°ƒç”¨ï¼Œå¸¦é‡è¯•æœºåˆ¶
		response, err := rc.callLLMWithRetry(ctx, client, request, 3)
		if err != nil {
			log.Printf("[ERROR] ReactCore: LLM call failed at iteration %d after retries: %v", iteration, err)
			if isStreaming {
				streamCallback(StreamChunk{Type: "error", Content: fmt.Sprintf("âŒ LLM call failed: %v", err)})
			}
			return nil, fmt.Errorf("LLM call failed at iteration %d: %w", iteration, err)
		}

		// å¢å¼ºçš„å“åº”éªŒè¯
		if response == nil {
			err := fmt.Errorf("received nil response from LLM at iteration %d", iteration)
			log.Printf("[ERROR] ReactCore: %v", err)
			if isStreaming {
				streamCallback(StreamChunk{Type: "error", Content: "âŒ Received empty response from LLM"})
			}
			return nil, err
		}

		if len(response.Choices) == 0 {
			log.Printf("[ERROR] ReactCore: No response choices at iteration %d", iteration)
			log.Printf("[DEBUG] ReactCore: Full response: %+v", response)
			if isStreaming {
				streamCallback(StreamChunk{Type: "error", Content: "âŒ No response choices from LLM - API response format issue"})
			}
			return nil, fmt.Errorf("no response choices received at iteration %d - API response format issue", iteration)
		}

		choice := response.Choices[0]
		step.Thought = strings.TrimSpace(choice.Message.Content)

		// æ·»åŠ assistantæ¶ˆæ¯åˆ°å¯¹è¯å†å²
		messages = append(messages, choice.Message)

		// è§£æå¹¶æ‰§è¡Œå·¥å…·è°ƒç”¨
		toolCalls := rc.agent.parseToolCalls(&choice.Message)
		if len(toolCalls) > 0 {
			step.Action = "tool_execution"
			step.ToolCall = toolCalls[0] // è®°å½•ç¬¬ä¸€ä¸ªå·¥å…·è°ƒç”¨

			if isStreaming {
				streamCallback(StreamChunk{
					Type:     "tool_start",
					Content:  fmt.Sprintf("âš¡ Executing %d tool(s): %s", len(toolCalls), rc.formatToolNames(toolCalls)),
					Metadata: map[string]interface{}{"iteration": iteration, "tools": rc.formatToolNames(toolCalls)}})
			}

			// æ‰§è¡Œå·¥å…·è°ƒç”¨
			var toolResult *types.ReactToolResult
			if isStreaming {
				toolResult = rc.agent.executeParallelToolsStream(ctx, toolCalls, streamCallback)
			} else {
				toolResult = rc.agent.executeParallelTools(ctx, toolCalls)
			}

			step.Result = toolResult

			// å°†å·¥å…·ç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²
			if toolResult != nil {
				toolMessages := rc.buildToolMessages(toolResult)
				messages = append(messages, toolMessages...)

				step.Observation = rc.generateObservation(toolResult, iteration)

				if isStreaming {
					streamCallback(StreamChunk{
						Type:     "tool_result",
						Content:  step.Observation,
						Metadata: map[string]interface{}{"iteration": iteration, "success": toolResult.Success}})
				}

				// æ£€æŸ¥æ˜¯å¦æ˜¯thinkå·¥å…·çš„ç»“æœï¼Œå¹¶è¯„ä¼°æ˜¯å¦éœ€è¦ç»§ç»­
				if rc.isThinkToolResult(toolResult) && rc.shouldContinueAfterThinking(toolResult.Content) {
					// Thinkå·¥å…·æ‰§è¡Œå®Œæˆï¼Œç»§ç»­ä¸‹ä¸€è½®
					log.Printf("[DEBUG] Think tool completed, continuing to next iteration")
				} else if rc.isTaskCompleteFromResult(toolResult, step.Thought) {
					// ä»»åŠ¡å®Œæˆ
					if isStreaming {
						streamCallback(StreamChunk{Type: "complete", Content: "âœ… Task completed successfully"})
					}

					step.Duration = time.Since(step.Timestamp)
					taskCtx.History = append(taskCtx.History, step)

					finalAnswer := rc.extractFinalAnswer(toolResult, step.Thought)
					return rc.buildFinalResult(taskCtx, finalAnswer, 0.9, true), nil
				}
			}
		} else {
			// æ— å·¥å…·è°ƒç”¨ï¼Œå¯èƒ½æ˜¯ç›´æ¥ç­”æ¡ˆ
			finalAnswer := choice.Message.Content

			if isStreaming {
				streamCallback(StreamChunk{
					Type:     "final_answer",
					Content:  finalAnswer,
					Metadata: map[string]interface{}{"iteration": iteration}})
				streamCallback(StreamChunk{Type: "complete", Content: "âœ… Task completed"})
			}

			step.Action = "direct_answer"
			step.Observation = finalAnswer
			step.Duration = time.Since(step.Timestamp)
			taskCtx.History = append(taskCtx.History, step)

			return rc.buildFinalResult(taskCtx, finalAnswer, 0.8, true), nil
		}

		step.Duration = time.Since(step.Timestamp)
		taskCtx.History = append(taskCtx.History, step)
		taskCtx.LastUpdate = time.Now()
	}

	// è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
	log.Printf("[WARN] ReactCore: Maximum iterations (%d) reached", maxIterations)
	if isStreaming {
		streamCallback(StreamChunk{
			Type:     "max_iterations",
			Content:  fmt.Sprintf("âš ï¸ Reached maximum iterations (%d)", maxIterations),
			Metadata: map[string]interface{}{"max_iterations": maxIterations}})
		streamCallback(StreamChunk{Type: "complete", Content: "âš ï¸ Maximum iterations reached"})
	}

	return rc.buildFinalResult(taskCtx, "Maximum iterations reached without completion", 0.5, false), nil
}

// buildToolDrivenTaskPrompt - æ„å»ºå·¥å…·é©±åŠ¨çš„ä»»åŠ¡æç¤º
func (rc *ReactCore) buildToolDrivenTaskPrompt(task string) string {
	// ä½¿ç”¨é¡¹ç›®å†…çš„prompt builder
	if rc.agent.promptBuilder != nil && rc.agent.promptBuilder.promptLoader != nil {
		// å°è¯•ä½¿ç”¨React thinking promptä½œä¸ºåŸºç¡€æ¨¡æ¿
		template, err := rc.agent.promptBuilder.promptLoader.GetReActThinkingPrompt()
		if err != nil {
			log.Printf("[WARN] ReactCore: Failed to get ReAct thinking prompt, trying fallback: %v", err)
		}
		// æ„å»ºå¢å¼ºçš„ä»»åŠ¡æç¤ºï¼Œå°†ç‰¹å®šä»»åŠ¡ä¿¡æ¯ä¸ReActæ¨¡æ¿ç»“åˆ
		return template
	}

	// Fallback to hardcoded prompt if prompt builder is not available
	log.Printf("[WARN] ReactCore: Prompt builder not available, using hardcoded prompt")
	return rc.buildHardcodedTaskPrompt(task)
}

// buildHardcodedTaskPrompt - æ„å»ºç¡¬ç¼–ç çš„ä»»åŠ¡æç¤ºï¼ˆfallbackï¼‰
func (rc *ReactCore) buildHardcodedTaskPrompt(task string) string {

	return fmt.Sprintf(`You are an intelligent agent with access to powerful tools. Your goal is to complete this task efficiently:

**time:** %s


**Approach:**
1. **For complex tasks**: Start with the 'think' tool to analyze and plan
2. **For multi-step tasks**: Use 'todo_update' to create structured task lists
3. **For file operations**: Use appropriate file tools (file_read, file_update, etc.)
4. **For system operations**: Use bash tool when needed
5. **For search/analysis**: Use grep or other search tools

**Think Tool Capabilities:**
- Phase: analyze, plan, reflect, reason, ultra_think
- Depth: shallow, normal, deep, ultra
- Use for strategic thinking and problem breakdown

**Todo Management:**
- todo_update: Create, batch create, update, complete tasks
- todo_read: Read current todos with filtering and statistics

**Guidelines:**
- Use the 'think' tool first for complex problems requiring analysis
- Break down multi-step tasks using todo_update
- Execute tools systematically to achieve the goal
- Provide clear, actionable results

Begin by determining the best approach for this task.`, time.Now().Format(time.RFC3339))
}

// buildToolDefinitions - æ„å»ºå·¥å…·å®šä¹‰åˆ—è¡¨ï¼ˆåŒ…æ‹¬thinkå·¥å…·ï¼‰
func (rc *ReactCore) buildToolDefinitions() []llm.Tool {
	var tools []llm.Tool

	for _, tool := range rc.agent.tools {
		toolDef := llm.Tool{
			Type: "function",
			Function: llm.Function{
				Name:        tool.Name(),
				Description: tool.Description(),
				Parameters:  tool.Parameters(),
			},
		}

		tools = append(tools, toolDef)
	}

	return tools
}

// buildToolMessages - æ„å»ºå·¥å…·ç»“æœæ¶ˆæ¯
func (rc *ReactCore) buildToolMessages(actionResult *types.ReactToolResult) []llm.Message {
	var toolMessages []llm.Message

	if len(actionResult.ToolCalls) > 0 {
		// å¤„ç†å¤šä¸ªå·¥å…·è°ƒç”¨çš„ç»“æœ
		for _, toolCall := range actionResult.ToolCalls {
			toolMessage := llm.Message{
				Role:       "assistant",
				Content:    actionResult.Content,
				ToolCallId: toolCall.CallID,
			}
			if !actionResult.Success {
				toolMessage.Content = actionResult.Error
			}
			toolMessages = append(toolMessages, toolMessage)
		}
	} else {
		// å¤„ç†å•ä¸ªå·¥å…·æˆ–ä»£ç æ‰§è¡Œç»“æœ
		content := actionResult.Content
		if !actionResult.Success {
			content = actionResult.Error
		}
		toolMessages = append(toolMessages, llm.Message{
			Role:    "assistant",
			Content: content,
		})
	}

	return toolMessages
}

// generateObservation - ç”Ÿæˆè§‚å¯Ÿç»“æœ
func (rc *ReactCore) generateObservation(toolResult *types.ReactToolResult, iteration int) string {
	if toolResult == nil {
		return "No tool execution result to observe"
	}

	if toolResult.Success {
		// æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹å®šå·¥å…·çš„ç»“æœ
		if len(toolResult.ToolCalls) > 0 {
			toolName := toolResult.ToolCalls[0].Name
			// æ¸…ç†å·¥å…·è¾“å‡ºï¼Œç§»é™¤å†—ä½™æ ¼å¼ä¿¡æ¯
			cleanContent := rc.cleanToolOutput(toolResult.Content)
			switch toolName {
			case "think":
				return fmt.Sprintf("ğŸ§  Thinking completed: %s", rc.truncateContent(cleanContent, 100))
			case "todo_update":
				return fmt.Sprintf("ğŸ“‹ Todo management: %s", rc.truncateContent(cleanContent, 100))
			case "file_read":
				return fmt.Sprintf("ğŸ“– File read: %s", rc.truncateContent(cleanContent, 100))
			case "bash":
				return fmt.Sprintf("âš¡ Command executed: %s", rc.truncateContent(cleanContent, 100))
			default:
				return fmt.Sprintf("âœ… %s completed: %s", toolName, rc.truncateContent(cleanContent, 100))
			}
		}
		return fmt.Sprintf("âœ… Tool execution successful: %s", rc.truncateContent(rc.cleanToolOutput(toolResult.Content), 100))
	} else {
		return fmt.Sprintf("âŒ Tool execution failed: %s", toolResult.Error)
	}
}

// formatToolNames - æ ¼å¼åŒ–å·¥å…·åç§°åˆ—è¡¨
func (rc *ReactCore) formatToolNames(toolCalls []*types.ReactToolCall) string {
	var names []string
	for _, tc := range toolCalls {
		names = append(names, tc.Name)
	}
	return strings.Join(names, ", ")
}

// isThinkToolResult - æ£€æŸ¥æ˜¯å¦æ˜¯thinkå·¥å…·çš„ç»“æœ
func (rc *ReactCore) isThinkToolResult(toolResult *types.ReactToolResult) bool {
	if toolResult == nil || len(toolResult.ToolCalls) == 0 {
		return false
	}
	return toolResult.ToolCalls[0].Name == "think"
}

// shouldContinueAfterThinking - åˆ¤æ–­æ€è€ƒåæ˜¯å¦åº”è¯¥ç»§ç»­
func (rc *ReactCore) shouldContinueAfterThinking(thinkingResult string) bool {
	// ç®€å•å¯å‘å¼ï¼šå¦‚æœthinkingç»“æœåŒ…å«actionè¯æ±‡ï¼Œåº”è¯¥ç»§ç»­æ‰§è¡Œ
	content := strings.ToLower(thinkingResult)
	actionWords := []string{"need to", "should", "next step", "implement", "create", "execute", "run", "call"}

	for _, word := range actionWords {
		if strings.Contains(content, word) {
			return true
		}
	}

	// å¦‚æœthinkingç»“æœå¾ˆé•¿ï¼Œå¯èƒ½åŒ…å«å®Œæ•´çš„åˆ†æï¼Œåº”è¯¥ç»§ç»­
	return len(thinkingResult) > 200
}

// isTaskCompleteFromResult - åŸºäºå·¥å…·ç»“æœåˆ¤æ–­ä»»åŠ¡æ˜¯å¦å®Œæˆ
func (rc *ReactCore) isTaskCompleteFromResult(toolResult *types.ReactToolResult, thought string) bool {
	if toolResult == nil {
		return false
	}

	content := strings.ToLower(toolResult.Content)

	// æ˜ç¡®çš„å®Œæˆä¿¡å·
	completionSignals := []string{
		"task completed", "successfully completed", "finished", "done",
		"implementation complete", "all tests pass", "deployment successful",
		"todo completed", "all todos completed",
	}

	for _, signal := range completionSignals {
		if strings.Contains(content, signal) {
			return true
		}
	}

	// æ£€æŸ¥æ˜¯å¦æ˜¯todoå·¥å…·å®Œæˆäº†æœ€åä¸€ä¸ªä»»åŠ¡
	if len(toolResult.ToolCalls) > 0 && toolResult.ToolCalls[0].Name == "todo_update" {
		if strings.Contains(content, "completed") &&
			(strings.Contains(strings.ToLower(thought), "final") || strings.Contains(strings.ToLower(thought), "last")) {
			return true
		}
	}

	return false
}

// extractFinalAnswer - ä»ç»“æœä¸­æå–æœ€ç»ˆç­”æ¡ˆ
func (rc *ReactCore) extractFinalAnswer(toolResult *types.ReactToolResult, thought string) string {
	if toolResult != nil && toolResult.Success {
		return toolResult.Content
	}
	return thought
}

// truncateContent - æˆªæ–­å†…å®¹åˆ°æŒ‡å®šé•¿åº¦
func (rc *ReactCore) truncateContent(content string, maxLen int) string {
	if maxLen <= 0 {
		return ""
	}
	if len(content) <= maxLen {
		return content
	}
	// ç¡®ä¿ä¸ä¼šè¶Šç•Œ
	if maxLen > len(content) {
		maxLen = len(content)
	}
	return content[:maxLen] + "..."
}

// buildFinalResult - æ„å»ºæœ€ç»ˆç»“æœ
func (rc *ReactCore) buildFinalResult(taskCtx *types.ReactTaskContext, answer string, confidence float64, success bool) *types.ReactTaskResult {
	totalDuration := time.Since(taskCtx.StartTime)

	return &types.ReactTaskResult{
		Success:    success,
		Answer:     answer,
		Confidence: confidence,
		Steps:      taskCtx.History,
		Duration:   totalDuration,
		TokensUsed: taskCtx.TokensUsed,
	}
}

// validateLLMRequest - éªŒè¯LLMè¯·æ±‚å‚æ•°
func (rc *ReactCore) validateLLMRequest(request *llm.ChatRequest) error {
	if request == nil {
		return fmt.Errorf("request is nil")
	}

	if len(request.Messages) == 0 {
		return fmt.Errorf("no messages in request")
	}

	if request.Config == nil {
		return fmt.Errorf("config is nil")
	}

	return nil
}

// callLLMWithRetry - å¸¦é‡è¯•æœºåˆ¶çš„æµå¼LLMè°ƒç”¨
func (rc *ReactCore) callLLMWithRetry(ctx context.Context, client llm.Client, request *llm.ChatRequest, maxRetries int) (*llm.ChatResponse, error) {
	var lastErr error

	for attempt := 1; attempt <= maxRetries; attempt++ {
		log.Printf("[DEBUG] ReactCore: LLM streaming call attempt %d/%d", attempt, maxRetries)
		log.Printf("[DEBUG] ReactCore: Messages count: %d", len(request.Messages))

		// ä½¿ç”¨æµå¼è°ƒç”¨
		streamChan, err := client.ChatStream(ctx, request)
		if err != nil {
			lastErr = err
			log.Printf("[WARN] ReactCore: Stream initialization failed (attempt %d): %v", attempt, err)

			// æ£€æŸ¥æ˜¯å¦æ˜¯500é”™è¯¯ï¼Œå¦‚æœæ˜¯ï¼Œè¯´æ˜è¯·æ±‚æ ¼å¼å¯èƒ½æœ‰é—®é¢˜ï¼Œä¸è¦é‡è¯•
			if strings.Contains(err.Error(), "500") {
				log.Printf("[ERROR] ReactCore: Server error 500, not retrying: %v", err)
				return nil, fmt.Errorf("server error 500 - request format issue: %w", err)
			}

			if attempt < maxRetries {
				backoffDuration := time.Duration(attempt*2) * time.Second
				log.Printf("[WARN] ReactCore: Retrying in %v", backoffDuration)
				select {
				case <-ctx.Done():
					return nil, ctx.Err()
				case <-time.After(backoffDuration):
					continue
				}
			}
			continue
		}

		// å¤„ç†æµå¼å“åº”å¹¶é‡æ„ä¸ºå®Œæ•´å“åº”
		response, err := rc.collectStreamingResponse(ctx, streamChan)
		if err == nil && response != nil {
			log.Printf("[DEBUG] ReactCore: Successfully collected streaming response")
			return response, nil
		}

		lastErr = err
		log.Printf("[WARN] ReactCore: Failed to collect streaming response (attempt %d): %v", attempt, err)

		if attempt < maxRetries {
			backoffDuration := time.Duration(attempt*2) * time.Second
			log.Printf("[WARN] ReactCore: Retrying in %v", backoffDuration)
			select {
			case <-ctx.Done():
				return nil, ctx.Err()
			case <-time.After(backoffDuration):
				continue
			}
		}
	}

	return nil, fmt.Errorf("streaming LLM call failed after %d attempts: %w", maxRetries, lastErr)
}

// collectStreamingResponse - æ”¶é›†æµå¼å“åº”å¹¶é‡æ„ä¸ºå®Œæ•´å“åº”
func (rc *ReactCore) collectStreamingResponse(ctx context.Context, streamChan <-chan llm.StreamDelta) (*llm.ChatResponse, error) {
	var response *llm.ChatResponse
	var contentBuilder strings.Builder
	var toolCalls []llm.ToolCall
	var currentToolCall *llm.ToolCall

	// æ£€æŸ¥æ˜¯å¦æœ‰æµå›è°ƒéœ€è¦é€šçŸ¥
	hasStreamCallback := rc.streamCallback != nil

	for {
		select {
		case <-ctx.Done():
			return nil, ctx.Err()
		case delta, ok := <-streamChan:
			if !ok {
				// æµç»“æŸï¼Œæ„å»ºæœ€ç»ˆå“åº”
				if response == nil {
					return nil, fmt.Errorf("no response received from stream")
				}

				// è®¾ç½®æœ€ç»ˆçš„æ¶ˆæ¯å†…å®¹
				if len(response.Choices) > 0 {
					response.Choices[0].Message.Content = contentBuilder.String()
					if len(toolCalls) > 0 {
						response.Choices[0].Message.ToolCalls = toolCalls
					}
				}

				log.Printf("[DEBUG] ReactCore: Collected complete response with %d chars, %d tool calls",
					contentBuilder.Len(), len(toolCalls))
				return response, nil
			}

			// åˆå§‹åŒ–å“åº”å¯¹è±¡
			if response == nil {
				response = &llm.ChatResponse{
					ID:      delta.ID,
					Object:  delta.Object,
					Created: delta.Created,
					Model:   delta.Model,
					Choices: make([]llm.Choice, 1),
				}
				response.Choices[0] = llm.Choice{
					Index: 0,
					Message: llm.Message{
						Role: "assistant",
					},
				}
			}

			// å¤„ç†æ¯ä¸ªdeltaä¸­çš„choice
			if len(delta.Choices) > 0 {
				choice := delta.Choices[0]

				// å¤„ç†å†…å®¹å¢é‡
				if choice.Delta.Content != "" {
					contentBuilder.WriteString(choice.Delta.Content)

					// å¦‚æœå¯ç”¨æµå¼ï¼Œå®æ—¶æ˜¾ç¤ºLLMè¾“å‡ºå†…å®¹
					if hasStreamCallback {
						rc.streamCallback(StreamChunk{
							Type:     "llm_content",
							Content:  choice.Delta.Content,
							Metadata: map[string]interface{}{"streaming": true},
						})
					}
				}

				// å¤„ç†å·¥å…·è°ƒç”¨å¢é‡
				if len(choice.Delta.ToolCalls) > 0 {
					for _, deltaToolCall := range choice.Delta.ToolCalls {
						if deltaToolCall.ID != "" {
							// æ–°çš„å·¥å…·è°ƒç”¨
							newToolCall := llm.ToolCall{
								ID:   deltaToolCall.ID,
								Type: deltaToolCall.Type,
								Function: llm.Function{
									Name:      deltaToolCall.Function.Name,
									Arguments: deltaToolCall.Function.Arguments,
								},
							}
							toolCalls = append(toolCalls, newToolCall)
							currentToolCall = &toolCalls[len(toolCalls)-1]
						} else if currentToolCall != nil {
							// ç»§ç»­ç°æœ‰å·¥å…·è°ƒç”¨
							if deltaToolCall.Function.Name != "" {
								currentToolCall.Function.Name += deltaToolCall.Function.Name
							}
							if deltaToolCall.Function.Arguments != "" {
								currentToolCall.Function.Arguments += deltaToolCall.Function.Arguments
							}
						}
					}
				}

				// æ£€æŸ¥å®ŒæˆåŸå› 
				if choice.FinishReason != "" {
					response.Choices[0].FinishReason = choice.FinishReason
				}
			}
		}
	}
}

// cleanToolOutput - æ¸…ç†å·¥å…·è¾“å‡ºï¼Œåªä¿ç•™å·¥å…·è°ƒç”¨æ ¼å¼
func (rc *ReactCore) cleanToolOutput(content string) string {
	lines := strings.Split(content, "\n")
	var cleanLines []string

	for _, line := range lines {
		trimmedLine := strings.TrimSpace(line)

		// åªä¿ç•™ğŸ”§å·¥å…·è°ƒç”¨æ ¼å¼çš„è¡Œï¼Œå…¶ä»–æ ¼å¼çš„è¡Œéƒ½ç§»é™¤
		if strings.HasPrefix(trimmedLine, "ğŸ”§ ") {
			cleanLines = append(cleanLines, trimmedLine)
		}
	}

	// å¦‚æœæ²¡æœ‰æ‰¾åˆ°å·¥å…·è°ƒç”¨æ ¼å¼ï¼Œè¿”å›ç®€æ´çš„æ‘˜è¦
	if len(cleanLines) == 0 {
		return rc.truncateContent(content, 50)
	}

	return strings.Join(cleanLines, "\n")
}
