package agent

import (
	"context"
	"fmt"
	"log"
	"strings"
	"time"

	contextmgr "alex/internal/context"
	"alex/internal/llm"
	"alex/internal/session"
	"alex/pkg/types"
)

// ReactCore - ä½¿ç”¨å·¥å…·è°ƒç”¨æµç¨‹çš„ReactCoreæ ¸å¿ƒå®ç°
type ReactCore struct {
	agent          *ReactAgent
	streamCallback StreamCallback
	contextHandler *ContextHandler
	llmHandler     *LLMHandler
	toolHandler    *ToolHandler
	promptHandler  *PromptHandler
}

// NewReactCore - åˆ›å»ºReActæ ¸å¿ƒå®ä¾‹
func NewReactCore(agent *ReactAgent) *ReactCore {
	llmClient, err := llm.GetLLMInstance(llm.BasicModel)
	if err != nil {
		log.Printf("[ERROR] NewReactCore: Failed to get LLM instance: %v", err)
		llmClient = nil
	}

	return &ReactCore{
		agent:          agent,
		contextHandler: NewContextHandler(llmClient, agent.sessionManager),
		llmHandler:     NewLLMHandler(nil), // Will be set per request
		toolHandler:    NewToolHandler(agent.tools),
		promptHandler:  NewPromptHandler(agent.promptBuilder),
	}
}

// SolveTask - ä½¿ç”¨å·¥å…·è°ƒç”¨æµç¨‹çš„ç®€åŒ–ä»»åŠ¡è§£å†³æ–¹æ³•
func (rc *ReactCore) SolveTask(ctx context.Context, task string, streamCallback StreamCallback) (*types.ReactTaskResult, error) {
	// è®¾ç½®æµå›è°ƒ
	rc.streamCallback = streamCallback
	rc.llmHandler.streamCallback = streamCallback

	// è·å–å½“å‰ä¼šè¯
	sess := rc.contextHandler.getCurrentSession(ctx, rc.agent)
	if sess != nil {
		// æ£€æŸ¥å¹¶å¤„ç†ä¸Šä¸‹æ–‡æº¢å‡º
		if err := rc.contextHandler.handleContextOverflow(ctx, sess, streamCallback); err != nil {
			log.Printf("[WARNING] Context overflow handling failed: %v", err)
		}
	}

	// ç”Ÿæˆä»»åŠ¡ID
	taskID := generateTaskID()

	// åˆå§‹åŒ–ä»»åŠ¡ä¸Šä¸‹æ–‡
	taskCtx := types.NewReactTaskContext(taskID, task)

	// å†³å®šæ˜¯å¦ä½¿ç”¨æµå¼å¤„ç†
	isStreaming := streamCallback != nil
	if isStreaming {
		streamCallback(StreamChunk{Type: "status", Content: "ğŸ§  Starting tool-driven ReAct process...", Metadata: map[string]any{"phase": "initialization"}})
	}

	// æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ŒåŸºäºä¼šè¯å†å²
	systemPrompt := rc.promptHandler.buildToolDrivenTaskPrompt()
	messages := rc.contextHandler.buildMessagesFromSession(sess, task, systemPrompt)

	// æ‰§è¡Œå·¥å…·é©±åŠ¨çš„ReActå¾ªç¯
	maxIterations := 25 // å‡å°‘è¿­ä»£æ¬¡æ•°ï¼Œä¾èµ–æ™ºèƒ½å·¥å…·è°ƒç”¨

	for iteration := 1; iteration <= maxIterations; iteration++ {
		step := types.ReactExecutionStep{
			Number:    iteration,
			Timestamp: time.Now(),
		}

		if isStreaming {
			streamCallback(StreamChunk{
				Type:     "iteration",
				Content:  fmt.Sprintf("ğŸ”„ Iteration %d: Processing with tool-driven approach...", iteration),
				Metadata: map[string]any{"iteration": iteration, "phase": "tool_driven_processing"}})
		}

		// æ„å»ºå¯ç”¨å·¥å…·åˆ—è¡¨ - æ¯è½®éƒ½åŒ…å«å·¥å…·å®šä¹‰ä»¥ç¡®ä¿æ¨¡å‹èƒ½è°ƒç”¨å·¥å…·
		tools := rc.toolHandler.buildToolDefinitions()

		request := &llm.ChatRequest{
			Messages:   messages,
			ModelType:  llm.BasicModel,
			Tools:      tools,
			ToolChoice: "auto",
			Config:     rc.agent.llmConfig,
			MaxTokens:  12000,
		}

		// è·å–LLMå®ä¾‹
		client, err := llm.GetLLMInstance(llm.BasicModel)
		if err != nil {
			log.Printf("[ERROR] ReactCore: Failed to get LLM instance at iteration %d: %v", iteration, err)
			if isStreaming {
				streamCallback(StreamChunk{Type: "error", Content: fmt.Sprintf("âŒ LLM initialization failed: %v", err)})
			}
			return nil, fmt.Errorf("LLM initialization failed at iteration %d: %w", iteration, err)
		}

		// æ·»åŠ è¯·æ±‚å‚æ•°éªŒè¯
		if err := rc.llmHandler.validateLLMRequest(request); err != nil {
			log.Printf("[ERROR] ReactCore: Invalid LLM request at iteration %d: %v", iteration, err)
			if isStreaming {
				streamCallback(StreamChunk{Type: "error", Content: fmt.Sprintf("âŒ Invalid request: %v", err)})
			}
			return nil, fmt.Errorf("invalid LLM request at iteration %d: %w", iteration, err)
		}

		// æ‰§è¡ŒLLMè°ƒç”¨ï¼Œå¸¦é‡è¯•æœºåˆ¶
		response, err := rc.llmHandler.callLLMWithRetry(ctx, client, request, 3)
		if err != nil {
			log.Printf("[ERROR] ReactCore: LLM call failed at iteration %d after retries: %v", iteration, err)
			if isStreaming {
				streamCallback(StreamChunk{Type: "error", Content: fmt.Sprintf("âŒ LLM call failed: %v", err)})
			}
			return nil, fmt.Errorf("LLM call failed at iteration %d: %w", iteration, err)
		}

		// å¢å¼ºçš„å“åº”éªŒè¯
		if response == nil {
			err := fmt.Errorf("received nil response from LLM at iteration %d", iteration)
			log.Printf("[ERROR] ReactCore: %v", err)
			if isStreaming {
				streamCallback(StreamChunk{Type: "error", Content: "âŒ Received empty response from LLM"})
			}
			return nil, err
		}

		if len(response.Choices) == 0 {
			log.Printf("[ERROR] ReactCore: No response choices at iteration %d", iteration)
			if isStreaming {
				streamCallback(StreamChunk{Type: "error", Content: "âŒ No response choices from LLM - API response format issue"})
			}
			return nil, fmt.Errorf("no response choices received at iteration %d - API response format issue", iteration)
		}

		log.Printf("DEBUG: Response: %+v", response)
		choice := response.Choices[0]
		step.Thought = strings.TrimSpace(choice.Message.Content)
		// æ·»åŠ assistantæ¶ˆæ¯åˆ°å¯¹è¯å†å²
		if len(choice.Message.Content) > 0 {
			messages = append(messages, choice.Message)
		}
		// è§£æå¹¶æ‰§è¡Œå·¥å…·è°ƒç”¨
		toolCalls := rc.agent.parseToolCalls(&choice.Message)
		if len(toolCalls) > 0 {
			step.Action = "tool_execution"
			step.ToolCall = toolCalls[0] // è®°å½•ç¬¬ä¸€ä¸ªå·¥å…·è°ƒç”¨

			if isStreaming {
				streamCallback(StreamChunk{
					Type:     "tool_start",
					Content:  fmt.Sprintf("âš¡ Executing %d tool(s): %s", len(toolCalls), rc.toolHandler.formatToolNames(toolCalls)),
					Metadata: map[string]any{"iteration": iteration, "tools": rc.toolHandler.formatToolNames(toolCalls)}})
			}

			// æ‰§è¡Œå·¥å…·è°ƒç”¨
			toolResult := rc.agent.executeSerialToolsStream(ctx, toolCalls, streamCallback)

			step.Result = toolResult

			// å°†å·¥å…·ç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²
			if toolResult != nil {
				toolMessages := rc.toolHandler.buildToolMessages(toolResult)
				messages = append(messages, toolMessages...)

				step.Observation = rc.toolHandler.generateObservation(toolResult)
			}
		} else {
			finalAnswer := choice.Message.Content

			if isStreaming {
				streamCallback(StreamChunk{
					Type:     "final_answer",
					Content:  finalAnswer,
					Metadata: map[string]any{"iteration": iteration}})
				streamCallback(StreamChunk{Type: "complete", Content: "âœ… Task completed"})
			}

			step.Action = "direct_answer"
			step.Observation = finalAnswer
			step.Duration = time.Since(step.Timestamp)
			taskCtx.History = append(taskCtx.History, step)

			return buildFinalResult(taskCtx, finalAnswer, 0.8, true), nil
		}

		step.Duration = time.Since(step.Timestamp)
		taskCtx.History = append(taskCtx.History, step)
		taskCtx.LastUpdate = time.Now()
	}

	// è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
	log.Printf("[WARN] ReactCore: Maximum iterations (%d) reached", maxIterations)
	if isStreaming {
		streamCallback(StreamChunk{
			Type:     "max_iterations",
			Content:  fmt.Sprintf("âš ï¸ Reached maximum iterations (%d)", maxIterations),
			Metadata: map[string]any{"max_iterations": maxIterations}})
		streamCallback(StreamChunk{Type: "complete", Content: "âš ï¸ Maximum iterations reached"})
	}

	return buildFinalResult(taskCtx, "Maximum iterations reached without completion", 0.5, false), nil
}

// GetContextStats - è·å–ä¸Šä¸‹æ–‡ç»Ÿè®¡ä¿¡æ¯
func (rc *ReactCore) GetContextStats(sess *session.Session) *contextmgr.ContextStats {
	return rc.contextHandler.GetContextStats(sess)
}

// ForceContextSummarization - å¼ºåˆ¶è¿›è¡Œä¸Šä¸‹æ–‡æ€»ç»“
func (rc *ReactCore) ForceContextSummarization(ctx context.Context, sess *session.Session) (*contextmgr.ContextProcessingResult, error) {
	return rc.contextHandler.ForceContextSummarization(ctx, sess)
}

// RestoreFullContext - æ¢å¤å®Œæ•´ä¸Šä¸‹æ–‡
func (rc *ReactCore) RestoreFullContext(sess *session.Session, backupID string) error {
	return rc.contextHandler.RestoreFullContext(sess, backupID)
}
